# ============================================================
# BIBLIOMETRIC PIPELINE (Scopus CSV) — Islamic finance
# Scope: 2000–2025 (exclude <2000 and exclude 2026)
# Colab-ready: mounts Drive, reads MyDrive/scopus_db.csv,
# outputs figures/tables/networks/meta + ZIP in MyDrive.
# Charts: Plotly + Kaleido (no label overlap, paper-grade exports)
# ============================================================

# --- 0) Install dependencies (Colab) ---
!pip -q install plotly kaleido networkx pyvis openpyxl unidecode

# --- 1) Mount Google Drive + set paths ---
from google.colab import drive
drive.mount("/content/drive")

from pathlib import Path
import datetime as dt
import pytz

TZ = pytz.timezone("Europe/Rome")
RUN_TS = dt.datetime.now(TZ).strftime("%Y%m%d_%H%M%S")

CSV_PATH = Path("/content/drive/MyDrive/scopus_db.csv")
OUT_DIR  = Path(f"/content/drive/MyDrive/biblio_outputs_islamic_finance/{RUN_TS}")
FIG_DIR  = OUT_DIR / "figures"
TAB_DIR  = OUT_DIR / "tables"
NET_DIR  = OUT_DIR / "networks"
META_DIR = OUT_DIR / "meta"

for p in [OUT_DIR, FIG_DIR, TAB_DIR, NET_DIR, META_DIR]:
    p.mkdir(parents=True, exist_ok=True)

print("CSV:", CSV_PATH)
print("OUT:", OUT_DIR)

# --- 2) Imports ---
import pandas as pd
import numpy as np
import re
import json
import shutil
import subprocess
import sys
from hashlib import sha256

import networkx as nx
from itertools import combinations

from unidecode import unidecode
from pyvis.network import Network

import plotly.graph_objects as go
import plotly.express as px

# --- 3) Plot helper (paper-grade exports + no overlaps) ---
def _ensure_kaleido() -> bool:
    """
    Garantisce la disponibilità di Kaleido per export statico Plotly.
    In Colab può fallire per mismatch/versioni: prova un fix automatico una volta.
    """
    try:
        import kaleido  # noqa: F401
        return True
    except Exception:
        pass

    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "-U", "kaleido"])
        import kaleido  # noqa: F401
        return True
    except Exception:
        return False


def save_plotly(fig, basepath_no_ext, width=1600, height=900, scale=2, write_html=False):
    """
    Salva PNG + PDF + SVG (vettoriale) con layout pulito e automargin.
    write_html=True solo se vuoi anche la versione interattiva.
    Fallback robusto: se Kaleido non è disponibile/non compatibile, non interrompe la pipeline.
    """
    fig.update_layout(
        template="simple_white",
        margin=dict(l=120, r=40, t=80, b=120),
        font=dict(size=16),
        title=dict(x=0.5, xanchor="center"),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="left", x=0),
    )
    fig.update_xaxes(automargin=True)
    fig.update_yaxes(automargin=True)

    export_err = None
    for attempt in range(2):
        try:
            fig.write_image(str(basepath_no_ext) + ".png", width=width, height=height, scale=scale)
            fig.write_image(str(basepath_no_ext) + ".pdf", width=width, height=height, scale=scale)
            fig.write_image(str(basepath_no_ext) + ".svg", width=width, height=height, scale=scale)
            export_err = None
            break
        except Exception as e:
            export_err = e
            if attempt == 0 and _ensure_kaleido():
                continue
            break

    if write_html or export_err is not None:
        fig.write_html(str(basepath_no_ext) + ".html", include_plotlyjs="cdn")

    if export_err is not None:
        warn_path = Path(str(basepath_no_ext) + "_export_warning.txt")
        warn_path.write_text(
            "Static export (png/pdf/svg) skipped after retry. "
            f"Reason: {repr(export_err)}\n"
            "HTML version was saved as fallback.",
            encoding="utf-8",
        )
        print(f"WARNING: static export failed for {basepath_no_ext}. Saved HTML fallback.")

# --- 4) Other helpers ---
def sha256_file(path: Path) -> str:
    h = sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def norm_text(x) -> str:
    if pd.isna(x):
        return ""
    x = unidecode(str(x)).lower().strip()
    x = re.sub(r"\s+", " ", x)
    return x

def first_author(auth_str) -> str:
    if pd.isna(auth_str) or str(auth_str).strip() == "":
        return ""
    return norm_text(str(auth_str).split(";")[0].strip())

def split_keywords_field(s):
    s = "" if pd.isna(s) else str(s)
    parts = [p.strip() for p in s.split(";") if p.strip()]
    parts = [norm_text(p) for p in parts]
    return [p for p in parts if p != ""]

def top_counts(df_in: pd.DataFrame, col: str, n=25) -> pd.DataFrame:
    if col not in df_in.columns:
        return pd.DataFrame(columns=[col, "count"])
    s = df_in[col].fillna("").astype(str).str.strip()
    s = s[s != ""]
    out = s.value_counts().head(n).reset_index()
    out.columns = [col, "count"]
    return out

def truncate_label(s: str, maxlen: int = 38) -> str:
    s = str(s)
    return s if len(s) <= maxlen else (s[: maxlen - 1] + "…")


def compute_interest_insights(year_df: pd.DataFrame) -> dict:
    d = year_df.copy().sort_values("year")

    peak_row = d.loc[d["n_pubs"].idxmax()]
    peak_year = int(peak_row["year"])
    peak_value = int(peak_row["n_pubs"])

    yoy = d[["year", "yoy_pct", "n_pubs"]].copy()
    yoy = yoy[yoy["yoy_pct"].notna()].copy()

    top_pos = yoy.sort_values("yoy_pct", ascending=False).head(3)
    top_neg = yoy.sort_values("yoy_pct", ascending=True).head(3)

    def rows_to_list(df_in):
        return [
            {"year": int(r["year"]), "yoy_pct": float(r["yoy_pct"]), "n_pubs": int(r["n_pubs"])}
            for _, r in df_in.iterrows()
        ]

    return {
        "peak_year": peak_year,
        "peak_value": peak_value,
        "top_yoy_pos": rows_to_list(top_pos),
        "top_yoy_neg": rows_to_list(top_neg),
    }


def add_safe_annotation(fig, x, y, text, ax=30, ay=-35):
    safe_text = str(text)
    if len(safe_text) > 60:
        safe_text = safe_text[:59] + "…"
    fig.add_annotation(
        x=x,
        y=y,
        text=safe_text,
        showarrow=True,
        arrowhead=2,
        ax=ax,
        ay=ay,
        bgcolor="rgba(255,255,255,0.85)",
        bordercolor="rgba(0,0,0,0.15)",
        borderwidth=1,
        font=dict(size=14),
    )

# --- 5) Load CSV robustly ---
if not CSV_PATH.exists():
    raise FileNotFoundError(f"CSV not found at {CSV_PATH}")

encodings = ["utf-8-sig", "utf-8", "cp1252", "latin1"]
df = None
last_err = None
for enc in encodings:
    try:
        df = pd.read_csv(CSV_PATH, encoding=enc)
        print(f"Loaded with encoding={enc}. Rows={len(df)} Cols={len(df.columns)}")
        break
    except Exception as e:
        last_err = e
if df is None:
    raise last_err

df.columns = [c.strip() for c in df.columns]

# --- 6) Column mapping (Scopus export standard) ---
COL = {
    "EID": "EID",
    "DOI": "DOI",
    "YEAR": "Year",
    "TITLE": "Title",
    "AUTHORS": "Authors",
    "SOURCE": "Source title",
    "CITED_BY": "Cited by",
    "DOCTYPE": "Document Type",
    "AK": "Author Keywords",
    "IK": "Index Keywords",
    "ABSTRACT": "Abstract",
    "AFFILS": "Affiliations",
}

for k in ["YEAR", "TITLE"]:
    if COL[k] not in df.columns:
        raise ValueError(f"Missing required column '{COL[k]}'. Available columns: {list(df.columns)}")

optional = ["EID","DOI","AUTHORS","SOURCE","CITED_BY","DOCTYPE","AK","IK","ABSTRACT","AFFILS"]
for k in optional:
    if COL[k] not in df.columns:
        print(f"WARNING: optional column missing: {COL[k]}")

# --- 7) Deduplication (EID → DOI → fallback) ---
df["_year"] = pd.to_numeric(df[COL["YEAR"]], errors="coerce")
df = df[df["_year"].notna()].copy()
df["_year"] = df["_year"].astype(int)

df["_eid"] = df[COL["EID"]].fillna("").astype(str).str.strip() if COL["EID"] in df.columns else ""
df["_doi"] = df[COL["DOI"]].fillna("").astype(str).str.strip().str.lower() if COL["DOI"] in df.columns else ""
df["_t"]   = df[COL["TITLE"]].map(norm_text)
df["_a1"]  = df[COL["AUTHORS"]].map(first_author) if COL["AUTHORS"] in df.columns else ""

df["_eid"] = df["_eid"].replace({"nan": ""})
df["_doi"] = df["_doi"].replace({"nan": ""})

def make_key(r):
    eid = r["_eid"] if isinstance(r["_eid"], str) else ""
    doi = r["_doi"] if isinstance(r["_doi"], str) else ""
    if eid != "":
        return f"EID:{eid}"
    if doi != "":
        return f"DOI:{doi}"
    return f"FALL:{r['_t']}|{r['_year']}|{r['_a1']}"

df["_dedup_key"] = df.apply(make_key, axis=1)

n_raw = len(df)
df_clean = df.drop_duplicates(subset=["_dedup_key"]).copy()
n_dedup = len(df_clean)

qa_dedup = {
    "rows_raw_after_year_parse": int(n_raw),
    "rows_dedup": int(n_dedup),
    "dup_removed": int(n_raw - n_dedup),
    "year_min": int(df_clean["_year"].min()),
    "year_max": int(df_clean["_year"].max()),
}
print("QA dedup:", qa_dedup)

df_clean.to_csv(TAB_DIR / "dataset_clean_dedup.csv", index=False)

# --- 8) Scope filter (2000–2025) ---
YEAR_MIN_SCOPE = 2000
YEAR_MAX_SCOPE = 2025

df_scope = df_clean[(df_clean["_year"] >= YEAR_MIN_SCOPE) & (df_clean["_year"] <= YEAR_MAX_SCOPE)].copy()

qa_scope = {
    "scope_year_min": YEAR_MIN_SCOPE,
    "scope_year_max": YEAR_MAX_SCOPE,
    "rows_in_scope": int(len(df_scope)),
    "excluded_pre_2000": int((df_clean["_year"] < YEAR_MIN_SCOPE).sum()),
    "excluded_gt_2025": int((df_clean["_year"] > YEAR_MAX_SCOPE).sum()),
    "excluded_2026": int((df_clean["_year"] == 2026).sum()),
}
print("QA scope:", qa_scope)

df_scope.to_csv(TAB_DIR / "dataset_scope_2000_2025.csv", index=False)

# --- 9) Interest over time (pubs/year, YoY, CAGR) + Plotly figures ---
year_df = (
    df_scope.groupby("_year")
    .size()
    .rename("n_pubs")
    .reset_index()
    .rename(columns={"_year": "year"})
    .sort_values("year")
)
year_df["yoy_pct"] = year_df["n_pubs"].pct_change() * 100.0
year_df["ma3"] = year_df["n_pubs"].rolling(3).mean()

y0, y1 = int(year_df["year"].min()), int(year_df["year"].max())
n0 = float(year_df.loc[year_df["year"] == y0, "n_pubs"].iloc[0])
n1 = float(year_df.loc[year_df["year"] == y1, "n_pubs"].iloc[0])
span = max(1, y1 - y0)
cagr = (n1 / n0) ** (1 / span) - 1 if n0 > 0 else np.nan

kpi_interest = pd.DataFrame([{
    "year_min": y0,
    "year_max": y1,
    "n_docs_in_scope": int(len(df_scope)),
    "cagr_publications": cagr
}])

year_df.to_csv(TAB_DIR / "table_pubs_by_year_2000_2025.csv", index=False)
kpi_interest.to_csv(TAB_DIR / "table_kpi_interest_2000_2025.csv", index=False)

ins = compute_interest_insights(year_df)

# Figure: pubs/year + MA3
fig = go.Figure()
fig.add_trace(go.Scatter(x=year_df["year"], y=year_df["n_pubs"], mode="lines+markers", name="Publications"))
fig.add_trace(go.Scatter(x=year_df["year"], y=year_df["ma3"], mode="lines", name="3-year moving average"))
fig.add_trace(go.Scatter(
    x=[ins["peak_year"]], y=[ins["peak_value"]],
    mode="markers",
    name="Peak year",
    marker=dict(size=14, symbol="circle-open", line=dict(width=2)),
    showlegend=True,
))
fig.add_vline(x=ins["peak_year"], line_width=1, line_dash="dot")
fig.update_layout(
    title="Islamic finance — publications per year (2000–2025)",
    xaxis_title="Year",
    yaxis_title="Number of publications",
    hovermode="x unified",
)
add_safe_annotation(fig, ins["peak_year"], ins["peak_value"], f"Peak: {ins['peak_year']} (n={ins['peak_value']})")

kpi_txt = (
    f"Scope: 2000–2025<br>"
    f"Total docs: {int(kpi_interest['n_docs_in_scope'].iloc[0])}<br>"
    f"CAGR: {kpi_interest['cagr_publications'].iloc[0]:.3f}"
)
fig.add_annotation(
    x=YEAR_MIN_SCOPE,
    y=float(year_df["n_pubs"].max()),
    xanchor="left",
    yanchor="top",
    text=kpi_txt,
    showarrow=False,
    bgcolor="rgba(255,255,255,0.85)",
    bordercolor="rgba(0,0,0,0.15)",
    borderwidth=1,
    font=dict(size=14),
)
# Niente sovrapposizioni sull'asse x
fig.update_xaxes(tickmode="linear", tick0=YEAR_MIN_SCOPE, dtick=2, tickangle=-45)
save_plotly(fig, FIG_DIR / "fig_pubs_per_year_2000_2025", width=1700, height=850, scale=2)

# Figure: YoY
fig = go.Figure()
fig.add_trace(go.Scatter(x=year_df["year"], y=year_df["yoy_pct"], mode="lines+markers", name="YoY growth (%)"))
fig.add_hline(y=0, line_width=1)

spikes = pd.DataFrame(ins["top_yoy_pos"] + ins["top_yoy_neg"])
if len(spikes) > 0:
    spikes["direction"] = (["top_pos"] * len(ins["top_yoy_pos"])) + (["top_neg"] * len(ins["top_yoy_neg"]))
    spikes = spikes.sort_values(["direction", "yoy_pct"], ascending=[True, False])
else:
    spikes = pd.DataFrame(columns=["year", "yoy_pct", "n_pubs", "direction"])
spikes.to_csv(TAB_DIR / "table_top_yoy_spikes.csv", index=False)


def add_spike_points(points, name, ax, ay):
    if not points:
        return
    xs = [p["year"] for p in points]
    ys = [p["yoy_pct"] for p in points]
    fig.add_trace(go.Scatter(
        x=xs,
        y=ys,
        mode="markers",
        name=name,
        marker=dict(size=12, symbol="diamond-open", line=dict(width=2)),
        showlegend=True,
    ))
    for p in points:
        add_safe_annotation(fig, p["year"], p["yoy_pct"], f"{p['year']}: {p['yoy_pct']:.1f}%", ax=ax, ay=ay)


add_spike_points(ins["top_yoy_pos"], "Top YoY ↑", ax=30, ay=-40)
add_spike_points(ins["top_yoy_neg"], "Top YoY ↓", ax=30, ay=40)

fig.update_layout(
    title="Islamic finance — YoY growth (2000–2025)",
    xaxis_title="Year",
    yaxis_title="YoY growth (%)",
    hovermode="x unified",
)
fig.update_xaxes(tickmode="linear", tick0=YEAR_MIN_SCOPE, dtick=2, tickangle=-45)
save_plotly(fig, FIG_DIR / "fig_yoy_growth_2000_2025", width=1700, height=850, scale=2)

# Figure: top 10 absolute YoY changes (bar)
yoy10 = year_df[year_df["yoy_pct"].notna()].copy()
yoy10["abs_yoy"] = yoy10["yoy_pct"].abs()
yoy10 = yoy10.sort_values("abs_yoy", ascending=False).head(10)
yoy10["year"] = yoy10["year"].astype(int).astype(str)

fig = px.bar(
    yoy10,
    x="year",
    y="yoy_pct",
    title="Largest YoY changes in publications (Top 10) — 2000–2025",
    labels={"year": "Year", "yoy_pct": "YoY growth (%)"},
)
fig.update_xaxes(tickangle=-45)
save_plotly(fig, FIG_DIR / "fig_yoy_top10_abs_2000_2025", width=1600, height=850, scale=2)

print(kpi_interest)

# --- 10) Bibliometric base tables + one XLSX ---
top_journals = top_counts(df_scope, COL["SOURCE"], 25) if COL["SOURCE"] in df_scope.columns else pd.DataFrame()
top_doctype  = top_counts(df_scope, COL["DOCTYPE"], 25) if COL["DOCTYPE"] in df_scope.columns else pd.DataFrame()

top_authors = pd.DataFrame()
if COL["AUTHORS"] in df_scope.columns:
    authors = df_scope[COL["AUTHORS"]].fillna("").astype(str)
    expl = authors.str.split(";").explode().str.strip()
    expl = expl[expl != ""]
    top_authors = expl.value_counts().head(25).reset_index()
    top_authors.columns = ["author", "count"]

top_cited = pd.DataFrame()
if COL["CITED_BY"] in df_scope.columns:
    tmp = df_scope.copy()
    tmp["_cited_by"] = pd.to_numeric(tmp[COL["CITED_BY"]], errors="coerce").fillna(0).astype(int)
    keep_cols = [c for c in [COL["TITLE"], COL["YEAR"], COL["SOURCE"], COL["DOI"], COL["EID"]] if c in tmp.columns]
    keep_cols += ["_cited_by"]
    top_cited = tmp.sort_values("_cited_by", ascending=False)[keep_cols].head(25)

if len(top_journals): top_journals.to_csv(TAB_DIR / "table_top_journals.csv", index=False)
if len(top_doctype):  top_doctype.to_csv(TAB_DIR / "table_doc_types.csv", index=False)
if len(top_authors):  top_authors.to_csv(TAB_DIR / "table_top_authors.csv", index=False)
if len(top_cited):    top_cited.to_csv(TAB_DIR / "table_top_cited.csv", index=False)

xlsx_path = TAB_DIR / "tables_all.xlsx"
with pd.ExcelWriter(xlsx_path, engine="openpyxl") as w:
    kpi_interest.to_excel(w, sheet_name="kpi_interest", index=False)
    year_df.to_excel(w, sheet_name="pubs_by_year", index=False)
    if len(top_journals): top_journals.to_excel(w, sheet_name="top_journals", index=False)
    if len(top_doctype):  top_doctype.to_excel(w, sheet_name="doc_types", index=False)
    if len(top_authors):  top_authors.to_excel(w, sheet_name="top_authors", index=False)
    if len(top_cited):    top_cited.to_excel(w, sheet_name="top_cited", index=False)

print("Saved tables:", xlsx_path)

# --- 11) Keyword mapping (paper-friendly: bar + heatmaps) ---
has_ak = (COL["AK"] in df_scope.columns)
has_ik = (COL["IK"] in df_scope.columns)

kw_author = df_scope[COL["AK"]].map(split_keywords_field) if has_ak else pd.Series([[]] * len(df_scope), index=df_scope.index)
kw_index  = df_scope[COL["IK"]].map(split_keywords_field) if has_ik else pd.Series([[]] * len(df_scope), index=df_scope.index)
kw_all = (kw_author + kw_index).map(lambda lst: sorted(set(lst)))

kw_flat = kw_all.explode()
kw_flat = kw_flat[kw_flat.notna() & (kw_flat != "")]
kw_freq = kw_flat.value_counts().reset_index()
kw_freq.columns = ["keyword", "count"]
kw_freq.to_csv(TAB_DIR / "table_keyword_frequency.csv", index=False)

if len(kw_freq) == 0:
    print("No keywords found (Author Keywords / Index Keywords empty). Skipping keyword mapping.")
else:
    # --- Fig: Top keywords (no overlap: truncate labels + hover full label) ---
    topk = kw_freq.head(20).copy()
    topk["keyword_full"] = topk["keyword"]
    topk["keyword_disp"] = topk["keyword"].map(lambda x: truncate_label(x, 42))
    topk = topk.sort_values("count", ascending=True)

    fig = px.bar(
        topk, x="count", y="keyword_disp", orientation="h",
        title="Top keywords (2000–2025)",
        labels={"count": "Frequency", "keyword_disp": "Keyword"},
        hover_data={"keyword_full": True, "keyword_disp": False}
    )
    fig.update_layout(height=1000)
    save_plotly(fig, FIG_DIR / "fig_top_keywords_2000_2025", width=1700, height=1000, scale=2)

    # --- Keyword trends top 20 -> heatmap (super clean) ---
    TOP_N = 20
    top_keywords = set(kw_freq.head(TOP_N)["keyword"])

    rows = []
    for y, kws in zip(df_scope["_year"].astype(int).values, kw_all.values):
        for k in kws:
            if k in top_keywords:
                rows.append((y, k))

    trend_tbl = pd.DataFrame(rows, columns=["year", "keyword"])
    trend_tbl = trend_tbl.groupby(["year", "keyword"]).size().reset_index(name="count")
    trend_tbl.to_csv(TAB_DIR / "table_keyword_trends_top20_2000_2025.csv", index=False)

    heat = trend_tbl.pivot(index="keyword", columns="year", values="count").fillna(0)
    # ordinamento keywords per frequenza totale (leggibilità)
    heat = heat.loc[heat.sum(axis=1).sort_values(ascending=False).index]

    fig = px.imshow(
        heat,
        aspect="auto",
        title="Keyword trends (Top 20) — 2000–2025",
        labels=dict(x="Year", y="Keyword", color="Count"),
    )
    fig.update_xaxes(tickmode="linear", tick0=YEAR_MIN_SCOPE, dtick=2, tickangle=-45)
    fig.update_layout(height=1000)
    save_plotly(fig, FIG_DIR / "fig_keyword_trends_heatmap_top20_2000_2025", width=1900, height=1000, scale=2)

    # --- Co-occurrence edges (for network + for paper heatmap) ---
    MIN_KW_FREQ = 5
    valid_kw = set(kw_freq[kw_freq["count"] >= MIN_KW_FREQ]["keyword"])

    edge_counts = {}
    for kws in kw_all:
        kws = [k for k in kws if k in valid_kw]
        kws = sorted(set(kws))
        for a, b in combinations(kws, 2):
            edge_counts[(a, b)] = edge_counts.get((a, b), 0) + 1

    edges = pd.DataFrame([{"kw1": a, "kw2": b, "weight": w} for (a, b), w in edge_counts.items()])
    edges = edges.sort_values("weight", ascending=False)
    edges.to_csv(TAB_DIR / "keyword_cooccurrence_edges.csv", index=False)

    # --- Paper-grade co-occurrence heatmap (Top 25 keywords) ---
    TOP_COOC = 25
    top_kw_for_matrix = kw_freq.head(TOP_COOC)["keyword"].tolist()

    # build matrix
    idx = {k:i for i,k in enumerate(top_kw_for_matrix)}
    M = np.zeros((TOP_COOC, TOP_COOC), dtype=int)

    for (a, b), w in edge_counts.items():
        if a in idx and b in idx:
            i, j = idx[a], idx[b]
            M[i, j] = w
            M[j, i] = w

    cooc_df = pd.DataFrame(M, index=top_kw_for_matrix, columns=top_kw_for_matrix)
    cooc_df.to_csv(TAB_DIR / "keyword_cooccurrence_matrix_top25.csv", index=True)

    fig = px.imshow(
        cooc_df,
        aspect="auto",
        title="Keyword co-occurrence (Top 25) — 2000–2025",
        labels=dict(x="Keyword", y="Keyword", color="Co-occurrence"),
    )
    fig.update_layout(height=1100)
    # evita sovrapposizioni: ruota tick e aumenta margini (già in save_plotly)
    fig.update_xaxes(tickangle=-45)
    save_plotly(fig, FIG_DIR / "fig_keyword_cooccurrence_heatmap_top25_2000_2025", width=1900, height=1100, scale=2)

    # --- Interactive network (supplementary) ---
    MAX_EDGES = 400
    edges_net = edges.head(MAX_EDGES).copy()

    G = nx.Graph()
    for _, e in edges_net.iterrows():
        G.add_edge(e["kw1"], e["kw2"], weight=int(e["weight"]))

    freq_map = dict(zip(kw_freq["keyword"], kw_freq["count"]))
    for n in G.nodes:
        G.nodes[n]["size"] = int(freq_map.get(n, 1))

    net = Network(height="850px", width="100%", bgcolor="white", font_color="black")
    net.from_nx(G)
    for node in net.nodes:
        node_id = node["id"]
        node["value"] = int(G.nodes[node_id].get("size", 1))

    html_path = NET_DIR / "keyword_network_cooccurrence_2000_2025.html"
    net.write_html(str(html_path))
    print("Saved keyword network HTML:", html_path)

# --- 12) PRISMA-style counts + manifest ---
prisma = pd.DataFrame([{
    "identified_records_after_year_parse": int(n_raw),
    "duplicates_removed": int(n_raw - n_dedup),
    "records_after_dedup": int(n_dedup),
    "included_2000_2025": int(len(df_scope)),
    "excluded_pre_2000": int((df_clean["_year"] < 2000).sum()),
    "excluded_gt_2025": int((df_clean["_year"] > 2025).sum()),
}])
prisma_path = META_DIR / "prisma_counts_2000_2025.csv"
prisma.to_csv(prisma_path, index=False)

manifest = {
    "run_timestamp_rome": dt.datetime.now(TZ).isoformat(),
    "csv_path": str(CSV_PATH),
    "csv_sha256": sha256_file(CSV_PATH),
    "scope": {"year_min": YEAR_MIN_SCOPE, "year_max": YEAR_MAX_SCOPE},
    "qa_dedup": qa_dedup,
    "qa_scope": qa_scope,
    "outputs": {
        "out_dir": str(OUT_DIR),
        "figures_dir": str(FIG_DIR),
        "tables_dir": str(TAB_DIR),
        "networks_dir": str(NET_DIR),
        "meta_dir": str(META_DIR),
    },
}

manifest_path = META_DIR / "manifest.json"
manifest_path.write_text(json.dumps(manifest, indent=2), encoding="utf-8")

ins = compute_interest_insights(year_df)

def fmt_points(points):
    return ", ".join([f"{p['year']} ({p['yoy_pct']:.1f}%)" for p in points]) if points else "NA"

report_md = f"""# Bibliometric run — Islamic finance (Scopus CSV)

Run timestamp (Europe/Rome): {dt.datetime.now(TZ).isoformat()}
Scope: 2000–2025

Counts:
- Raw (after year parse): {qa_dedup['rows_raw_after_year_parse']}
- Deduplicated: {qa_dedup['rows_dedup']} (removed {qa_dedup['dup_removed']})
- In scope (2000–2025): {qa_scope['rows_in_scope']}

Interest over time:
- Peak year: {ins['peak_year']} (n={ins['peak_value']})
- CAGR (2000–2025): {kpi_interest['cagr_publications'].iloc[0]:.4f}
- Top YoY ↑: {fmt_points(ins['top_yoy_pos'])}
- Top YoY ↓: {fmt_points(ins['top_yoy_neg'])}

Outputs:
- OUT_DIR: {OUT_DIR}
- ZIP: {str(OUT_DIR)}.zip
"""
(META_DIR / "report.md").write_text(report_md, encoding="utf-8")

# --- 13) ZIP all outputs ---
zip_path = shutil.make_archive(str(OUT_DIR), "zip", str(OUT_DIR))
print("Wrote manifest:", manifest_path)
print("Wrote PRISMA counts:", prisma_path)
print("ZIP:", zip_path)
print("DONE ✅")

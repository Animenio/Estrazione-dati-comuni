{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vigone Municipality Data Harvester\n",
    "\n",
    "## Purpose\n",
    "This notebook orchestrates an end-to-end pipeline for extracting municipal indicators from Vigone (TO) website PDFs.\n",
    "\n",
    "## Prerequisites\n",
    "- **Google Drive Mount**: Required for accessing CSV templates and storing outputs\n",
    "- **Templates**: 6 CSV indicator templates must exist in Drive at configured path\n",
    "- **System**: Runs on Google Colab with sufficient disk space for PDF processing\n",
    "\n",
    "## Configuration\n",
    "- `ANNO_TARGET`: Target year for data extraction (default: 2024)\n",
    "- `MAX_PAGES`: Web crawl depth limit (default: 50)\n",
    "- `MAX_PDFS`: Maximum PDFs to process (default: 30)\n",
    "\n",
    "## Output Structure\n",
    "```\n",
    "/content/drive/MyDrive/vigone_extraction/\n",
    "‚îú‚îÄ‚îÄ docs/          # Downloaded PDFs\n",
    "‚îú‚îÄ‚îÄ marker/        # Converted JSON/Markdown\n",
    "‚îú‚îÄ‚îÄ output/        # Populated CSV templates\n",
    "‚îî‚îÄ‚îÄ manifest.json  # Resume state tracking\n",
    "```\n",
    "\n",
    "**Workflow**: Web Discovery ‚Üí PDF Download ‚Üí Marker Conversion ‚Üí Indicator Extraction ‚Üí CSV Population ‚Üí Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install -q requests beautifulsoup4 pandas tqdm tenacity marker-pdf pytesseract pillow\n",
    "!apt-get install -y -qq poppler-utils tesseract-ocr tesseract-ocr-ita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters and directory setup\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Core configuration\n",
    "vigone_year = 2024\n",
    "vigone_page_limit = 50\n",
    "vigone_pdf_cap = 30\n",
    "vigone_origin = \"https://www.comune.vigone.to.it/\"\n",
    "\n",
    "# Path configuration\n",
    "vigone_workspace = Path(\"/content/drive/MyDrive/vigone_extraction\")\n",
    "vigone_pdf_storage = vigone_workspace / \"docs\"\n",
    "vigone_converted_storage = vigone_workspace / \"marker\"\n",
    "vigone_results_storage = vigone_workspace / \"output\"\n",
    "vigone_template_source = Path(\"/content/drive/MyDrive/templates\")\n",
    "vigone_state_file = vigone_workspace / \"manifest.json\"\n",
    "\n",
    "# Create directory structure\n",
    "for dir_path in [vigone_workspace, vigone_pdf_storage, vigone_converted_storage, vigone_results_storage]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Configured for year {vigone_year}\")\n",
    "print(f\"‚úì Directories ready at {vigone_workspace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Callable, Any, Set\n",
    "from functools import reduce, partial, wraps\n",
    "from itertools import islice, chain, groupby\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: hashing, normalization, extraction\n",
    "\n",
    "def stream_hash_digest(byte_stream: bytes) -> str:\n",
    "    \"\"\"Generate compact SHA256 digest for deduplication.\"\"\"\n",
    "    digest_engine = hashlib.sha256()\n",
    "    digest_engine.update(byte_stream)\n",
    "    return digest_engine.hexdigest()[:16]\n",
    "\n",
    "def text_normalization_chain(raw_text: str) -> str:\n",
    "    \"\"\"Apply multi-stage text normalization pipeline.\"\"\"\n",
    "    # Stage 1: Unicode normalization\n",
    "    stage1 = raw_text.lower()\n",
    "    # Stage 2: Whitespace collapsing\n",
    "    stage2 = re.sub(r'\\s+', ' ', stage1)\n",
    "    # Stage 3: Special character filtering\n",
    "    stage3 = re.sub(r'[^\\w\\s\\.,;:‚Ç¨$%()\\[\\]\\-/]', '', stage2)\n",
    "    # Stage 4: Trim edges\n",
    "    return stage3.strip()\n",
    "\n",
    "def url_canonicalization(href: str, base_domain: str) -> Optional[str]:\n",
    "    \"\"\"Canonicalize URL and validate against base domain.\"\"\"\n",
    "    try:\n",
    "        absolute_url = urljoin(base_domain, href)\n",
    "        url_components = urlparse(absolute_url)\n",
    "        domain_components = urlparse(base_domain)\n",
    "        \n",
    "        # Verify domain matching\n",
    "        if url_components.netloc == domain_components.netloc:\n",
    "            return absolute_url\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def numeric_pattern_extraction(text_fragment: str) -> Optional[float]:\n",
    "    \"\"\"Extract numeric values with European format handling.\"\"\"\n",
    "    # Remove all non-numeric except separators\n",
    "    sanitized = re.sub(r'[^\\d,\\.\\-]', '', text_fragment)\n",
    "    \n",
    "    # Handle European format: 1.234,56 -> 1234.56\n",
    "    if ',' in sanitized and '.' in sanitized:\n",
    "        # European format detected\n",
    "        sanitized = sanitized.replace('.', '').replace(',', '.')\n",
    "    elif ',' in sanitized:\n",
    "        # Only comma - treat as decimal\n",
    "        sanitized = sanitized.replace(',', '.')\n",
    "    \n",
    "    # Extract first numeric match\n",
    "    pattern_match = re.search(r'-?\\d+(?:\\.\\d+)?', sanitized)\n",
    "    if pattern_match:\n",
    "        try:\n",
    "            return float(pattern_match.group())\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def unit_inference_from_context(surrounding_text: str) -> str:\n",
    "    \"\"\"Infer measurement units from textual context.\"\"\"\n",
    "    text_lower = surrounding_text.lower()\n",
    "    \n",
    "    # Unit detection patterns (order matters - most specific first)\n",
    "    unit_rules = [\n",
    "        (r'\\b(euro|eur|‚Ç¨)\\b', '‚Ç¨'),\n",
    "        (r'\\b(percentual[ei]|percent[oi]|%)\\b', '%'),\n",
    "        (r'\\b(chilogramm[io]|kg)\\b', 'kg'),\n",
    "        (r'\\b(metri|metro|\\bm\\b)\\b', 'm'),\n",
    "        (r'\\b(abitant[ei]|resident[ei])\\b', 'persone'),\n",
    "        (r'\\b(giorn[io]|giorni)\\b', 'giorni'),\n",
    "    ]\n",
    "    \n",
    "    for pattern, unit in unit_rules:\n",
    "        if re.search(pattern, text_lower):\n",
    "            return unit\n",
    "    \n",
    "    return ''\n",
    "\n",
    "print(\"‚úì Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State persistence: JSON manifest for resume capability\n",
    "\n",
    "def read_processing_manifest(manifest_location: Path) -> Dict:\n",
    "    \"\"\"Load existing processing state or initialize new.\"\"\"\n",
    "    if manifest_location.exists():\n",
    "        try:\n",
    "            with open(manifest_location, 'r', encoding='utf-8') as stream:\n",
    "                return json.load(stream)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Initialize fresh state\n",
    "    return {\n",
    "        'url_catalog': [],\n",
    "        'pdf_registry': {},\n",
    "        'conversion_log': [],\n",
    "        'extraction_history': [],\n",
    "        'last_update': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def write_processing_manifest(manifest_location: Path, state_data: Dict) -> None:\n",
    "    \"\"\"Atomically write manifest state to disk.\"\"\"\n",
    "    state_data['last_update'] = datetime.now().isoformat()\n",
    "    \n",
    "    temp_location = manifest_location.with_suffix('.tmp')\n",
    "    with open(temp_location, 'w', encoding='utf-8') as stream:\n",
    "        json.dump(state_data, stream, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Atomic rename\n",
    "    temp_location.replace(manifest_location)\n",
    "\n",
    "def modify_manifest_attribute(manifest_location: Path, attribute_key: str, attribute_value: Any) -> None:\n",
    "    \"\"\"Update single manifest attribute.\"\"\"\n",
    "    current_state = read_processing_manifest(manifest_location)\n",
    "    current_state[attribute_key] = attribute_value\n",
    "    write_processing_manifest(manifest_location, current_state)\n",
    "\n",
    "print(\"‚úì State persistence functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web discovery pipeline: BFS crawler for PDF links\n",
    "\n",
    "def construct_web_crawler(origin_url: str, depth_boundary: int) -> Callable:\n",
    "    \"\"\"Build BFS-based web crawler closure.\"\"\"\n",
    "    \n",
    "    def crawl_and_discover() -> List[str]:\n",
    "        visited_urls = set()\n",
    "        discovered_pdfs = []\n",
    "        traversal_queue = deque([(origin_url, 0)])\n",
    "        \n",
    "        progress_tracker = tqdm(total=depth_boundary, desc=\"üîç Web Discovery\")\n",
    "        \n",
    "        while traversal_queue and len(visited_urls) < depth_boundary:\n",
    "            current_node, current_depth = traversal_queue.popleft()\n",
    "            \n",
    "            # Skip visited or too deep\n",
    "            if current_node in visited_urls or current_depth > 3:\n",
    "                continue\n",
    "            \n",
    "            visited_urls.add(current_node)\n",
    "            progress_tracker.update(1)\n",
    "            \n",
    "            try:\n",
    "                # Fetch page with timeout\n",
    "                http_response = requests.get(current_node, timeout=10, headers={\n",
    "                    'User-Agent': 'Mozilla/5.0 (compatible; VigoneDataBot/1.0)'\n",
    "                })\n",
    "                \n",
    "                if http_response.status_code != 200:\n",
    "                    continue\n",
    "                \n",
    "                # Parse HTML structure\n",
    "                page_soup = BeautifulSoup(http_response.content, 'html.parser')\n",
    "                \n",
    "                # Process all hyperlinks\n",
    "                for anchor_element in page_soup.find_all('a', href=True):\n",
    "                    link_href = anchor_element['href']\n",
    "                    \n",
    "                    # Identify PDF links\n",
    "                    if link_href.lower().endswith('.pdf'):\n",
    "                        canonical_url = url_canonicalization(link_href, origin_url)\n",
    "                        if canonical_url and canonical_url not in discovered_pdfs:\n",
    "                            discovered_pdfs.append(canonical_url)\n",
    "                    else:\n",
    "                        # Queue HTML pages for crawling\n",
    "                        canonical_url = url_canonicalization(link_href, origin_url)\n",
    "                        if canonical_url and canonical_url not in visited_urls:\n",
    "                            traversal_queue.append((canonical_url, current_depth + 1))\n",
    "                \n",
    "                # Polite crawling delay\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as crawl_error:\n",
    "                continue\n",
    "        \n",
    "        progress_tracker.close()\n",
    "        return discovered_pdfs\n",
    "    \n",
    "    return crawl_and_discover\n",
    "\n",
    "print(\"‚úì Web discovery pipeline configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF acquisition pipeline: download with retry and deduplication\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=15))\n",
    "def retrieve_document_bytes(document_url: str) -> bytes:\n",
    "    \"\"\"Fetch remote document with exponential backoff.\"\"\"\n",
    "    http_response = requests.get(document_url, timeout=30, headers={\n",
    "        'User-Agent': 'Mozilla/5.0 (compatible; VigoneDataBot/1.0)'\n",
    "    })\n",
    "    http_response.raise_for_status()\n",
    "    return http_response.content\n",
    "\n",
    "def construct_pdf_downloader(storage_path: Path, state_path: Path) -> Callable:\n",
    "    \"\"\"Build PDF acquisition pipeline with hash-based deduplication.\"\"\"\n",
    "    \n",
    "    def download_pdf_collection(url_collection: List[str], quantity_limit: int) -> Dict[str, str]:\n",
    "        current_state = read_processing_manifest(state_path)\n",
    "        hash_registry = current_state.get('pdf_registry', {})\n",
    "        \n",
    "        download_progress = tqdm(\n",
    "            islice(url_collection, quantity_limit),\n",
    "            desc=\"üì• Downloading PDFs\",\n",
    "            total=min(len(url_collection), quantity_limit)\n",
    "        )\n",
    "        \n",
    "        for pdf_url in download_progress:\n",
    "            try:\n",
    "                # Fetch document content\n",
    "                pdf_bytes = retrieve_document_bytes(pdf_url)\n",
    "                content_hash = stream_hash_digest(pdf_bytes)\n",
    "                \n",
    "                # Check for duplicate content\n",
    "                if content_hash in hash_registry.values():\n",
    "                    download_progress.set_postfix({'status': 'duplicate'})\n",
    "                    continue\n",
    "                \n",
    "                # Generate storage filename\n",
    "                storage_filename = f\"vigone_{content_hash}.pdf\"\n",
    "                storage_location = storage_path / storage_filename\n",
    "                \n",
    "                # Write to disk\n",
    "                with open(storage_location, 'wb') as output_stream:\n",
    "                    output_stream.write(pdf_bytes)\n",
    "                \n",
    "                # Register in hash map\n",
    "                hash_registry[pdf_url] = content_hash\n",
    "                download_progress.set_postfix({'status': 'saved'})\n",
    "                \n",
    "            except Exception as download_error:\n",
    "                download_progress.set_postfix({'status': 'failed'})\n",
    "                continue\n",
    "        \n",
    "        # Persist updated registry\n",
    "        modify_manifest_attribute(state_path, 'pdf_registry', hash_registry)\n",
    "        return hash_registry\n",
    "    \n",
    "    return download_pdf_collection\n",
    "\n",
    "print(\"‚úì PDF acquisition pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker conversion pipeline: PDF to JSON/Markdown\n",
    "\n",
    "def construct_marker_transformer(source_path: Path, target_path: Path, state_path: Path) -> Callable:\n",
    "    \"\"\"Build Marker-based PDF transformation pipeline.\"\"\"\n",
    "    \n",
    "    def transform_pdf_to_structured() -> List[Dict]:\n",
    "        current_state = read_processing_manifest(state_path)\n",
    "        conversion_registry = current_state.get('conversion_log', [])\n",
    "        transformation_results = []\n",
    "        \n",
    "        pdf_inventory = list(source_path.glob('*.pdf'))\n",
    "        conversion_progress = tqdm(pdf_inventory, desc=\"üîÑ Marker Conversion\")\n",
    "        \n",
    "        for pdf_file in conversion_progress:\n",
    "            json_target = target_path / f\"{pdf_file.stem}.json\"\n",
    "            markdown_target = target_path / f\"{pdf_file.stem}.md\"\n",
    "            \n",
    "            # Skip already processed\n",
    "            if str(pdf_file) in conversion_registry:\n",
    "                if json_target.exists():\n",
    "                    try:\n",
    "                        with open(json_target, 'r', encoding='utf-8') as json_stream:\n",
    "                            transformation_results.append(json.load(json_stream))\n",
    "                    except:\n",
    "                        pass\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Execute marker conversion\n",
    "                import subprocess\n",
    "                conversion_command = [\n",
    "                    'marker_single',\n",
    "                    str(pdf_file),\n",
    "                    str(target_path),\n",
    "                    '--batch_multiplier', '2',\n",
    "                    '--langs', 'Italian'\n",
    "                ]\n",
    "                \n",
    "                subprocess.run(\n",
    "                    conversion_command,\n",
    "                    check=True,\n",
    "                    capture_output=True,\n",
    "                    text=True\n",
    "                )\n",
    "                \n",
    "                # Load conversion output\n",
    "                if json_target.exists():\n",
    "                    with open(json_target, 'r', encoding='utf-8') as json_stream:\n",
    "                        structured_data = json.load(json_stream)\n",
    "                        transformation_results.append(structured_data)\n",
    "                        conversion_registry.append(str(pdf_file))\n",
    "                        conversion_progress.set_postfix({'status': 'converted'})\n",
    "                \n",
    "            except Exception as conversion_error:\n",
    "                conversion_progress.set_postfix({'status': 'failed'})\n",
    "                continue\n",
    "        \n",
    "        # Update manifest\n",
    "        modify_manifest_attribute(state_path, 'conversion_log', conversion_registry)\n",
    "        return transformation_results\n",
    "    \n",
    "    return transform_pdf_to_structured\n",
    "\n",
    "print(\"‚úì Marker conversion pipeline configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator extraction pipeline: fuzzy search and regex parsing\n",
    "\n",
    "def calculate_token_similarity(sequence_a: str, sequence_b: str) -> float:\n",
    "    \"\"\"Custom token-based Jaccard similarity.\"\"\"\n",
    "    tokens_a = set(sequence_a.lower().split())\n",
    "    tokens_b = set(sequence_b.lower().split())\n",
    "    \n",
    "    if not tokens_a or not tokens_b:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_size = len(tokens_a & tokens_b)\n",
    "    union_size = len(tokens_a | tokens_b)\n",
    "    \n",
    "    return intersection_size / union_size if union_size > 0 else 0.0\n",
    "\n",
    "def construct_indicator_matcher(converted_path: Path, template_path: Path, year_target: int) -> Callable:\n",
    "    \"\"\"Build indicator extraction engine with fuzzy matching.\"\"\"\n",
    "    \n",
    "    def match_indicators_in_corpus() -> Dict[str, List[Dict]]:\n",
    "        # Load CSV templates\n",
    "        template_catalog = {}\n",
    "        for template_file in template_path.glob('*.csv'):\n",
    "            try:\n",
    "                template_df = pd.read_csv(template_file)\n",
    "                template_catalog[template_file.stem] = template_df\n",
    "            except Exception as load_error:\n",
    "                continue\n",
    "        \n",
    "        if not template_catalog:\n",
    "            print(\"‚ö†Ô∏è No templates found in\", template_path)\n",
    "            return {}\n",
    "        \n",
    "        # Build text corpus from markdown files\n",
    "        document_corpus = []\n",
    "        for markdown_file in converted_path.glob('*.md'):\n",
    "            try:\n",
    "                with open(markdown_file, 'r', encoding='utf-8') as md_stream:\n",
    "                    raw_content = md_stream.read()\n",
    "                    normalized_content = text_normalization_chain(raw_content)\n",
    "                    document_corpus.append(normalized_content)\n",
    "            except Exception as read_error:\n",
    "                continue\n",
    "        \n",
    "        extraction_results = defaultdict(list)\n",
    "        \n",
    "        # Process each template\n",
    "        for template_name, template_data in template_catalog.items():\n",
    "            print(f\"\\nüìä Processing template: {template_name}\")\n",
    "            \n",
    "            # Find indicator column\n",
    "            indicator_column = None\n",
    "            for column_name in template_data.columns:\n",
    "                column_lower = column_name.lower()\n",
    "                if 'indicat' in column_lower or 'descri' in column_lower or 'nome' in column_lower:\n",
    "                    indicator_column = column_name\n",
    "                    break\n",
    "            \n",
    "            if not indicator_column:\n",
    "                print(f\"  ‚ö†Ô∏è No indicator column found\")\n",
    "                continue\n",
    "            \n",
    "            # Match each indicator\n",
    "            for row_index, row_data in template_data.iterrows():\n",
    "                indicator_label = str(row_data[indicator_column])\n",
    "                \n",
    "                if pd.isna(indicator_label) or not indicator_label.strip():\n",
    "                    continue\n",
    "                \n",
    "                normalized_label = text_normalization_chain(indicator_label)\n",
    "                optimal_match = None\n",
    "                optimal_score = 0.0\n",
    "                \n",
    "                # Search in corpus\n",
    "                for document_text in document_corpus:\n",
    "                    # Split into semantic chunks (sentences)\n",
    "                    text_chunks = re.split(r'[.!?\\n]+', document_text)\n",
    "                    \n",
    "                    for chunk in text_chunks:\n",
    "                        if len(chunk) < 10:  # Skip too short\n",
    "                            continue\n",
    "                        \n",
    "                        similarity = calculate_token_similarity(normalized_label, chunk)\n",
    "                        \n",
    "                        if similarity > optimal_score and similarity > 0.3:\n",
    "                            optimal_score = similarity\n",
    "                            optimal_match = chunk\n",
    "                \n",
    "                # Extract numeric data from match\n",
    "                if optimal_match:\n",
    "                    extracted_value = numeric_pattern_extraction(optimal_match)\n",
    "                    inferred_unit = unit_inference_from_context(optimal_match)\n",
    "                    \n",
    "                    extraction_results[template_name].append({\n",
    "                        'row_idx': row_index,\n",
    "                        'indicator_name': indicator_label,\n",
    "                        'extracted_value': extracted_value,\n",
    "                        'unit': inferred_unit,\n",
    "                        'confidence_score': round(optimal_score, 3),\n",
    "                        'source_snippet': optimal_match[:250]\n",
    "                    })\n",
    "        \n",
    "        return dict(extraction_results)\n",
    "    \n",
    "    return match_indicators_in_corpus\n",
    "\n",
    "print(\"‚úì Indicator extraction pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV population pipeline: maintain template structure\n",
    "\n",
    "def construct_csv_writer(template_source: Path, output_target: Path, year_target: int) -> Callable:\n",
    "    \"\"\"Build CSV template population engine.\"\"\"\n",
    "    \n",
    "    def populate_template_csvs(extraction_map: Dict[str, List[Dict]]) -> List[Path]:\n",
    "        populated_files = []\n",
    "        \n",
    "        for template_identifier, extracted_indicators in extraction_map.items():\n",
    "            template_location = template_source / f\"{template_identifier}.csv\"\n",
    "            \n",
    "            if not template_location.exists():\n",
    "                print(f\"‚ö†Ô∏è Template not found: {template_identifier}\")\n",
    "                continue\n",
    "            \n",
    "            # Load template structure\n",
    "            template_df = pd.read_csv(template_location)\n",
    "            \n",
    "            # Locate or create year column\n",
    "            year_column_name = None\n",
    "            for column_name in template_df.columns:\n",
    "                if str(year_target) in str(column_name) or 'anno' in str(column_name).lower():\n",
    "                    year_column_name = column_name\n",
    "                    break\n",
    "            \n",
    "            if not year_column_name:\n",
    "                # Create new year column\n",
    "                year_column_name = f\"Anno_{year_target}\"\n",
    "                template_df[year_column_name] = None\n",
    "            \n",
    "            # Inject extracted values\n",
    "            for extraction_item in extracted_indicators:\n",
    "                target_row = extraction_item['row_idx']\n",
    "                extracted_val = extraction_item['extracted_value']\n",
    "                \n",
    "                if extracted_val is not None and target_row < len(template_df):\n",
    "                    template_df.at[target_row, year_column_name] = extracted_val\n",
    "            \n",
    "            # Write populated CSV\n",
    "            output_location = output_target / f\"{template_identifier}_vigone_{year_target}.csv\"\n",
    "            template_df.to_csv(output_location, index=False)\n",
    "            populated_files.append(output_location)\n",
    "            print(f\"‚úÖ Populated: {template_identifier}\")\n",
    "        \n",
    "        return populated_files\n",
    "    \n",
    "    return populate_template_csvs\n",
    "\n",
    "print(\"‚úì CSV population pipeline configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporting pipeline: JSON report with stats and confidence scores\n",
    "\n",
    "def construct_report_builder(output_location: Path) -> Callable:\n",
    "    \"\"\"Build comprehensive reporting engine.\"\"\"\n",
    "    \n",
    "    def build_extraction_report(\n",
    "        extraction_map: Dict[str, List[Dict]],\n",
    "        pdf_quantity: int,\n",
    "        url_quantity: int\n",
    "    ) -> Dict:\n",
    "        \n",
    "        report_structure = {\n",
    "            'generation_timestamp': datetime.now().isoformat(),\n",
    "            'pipeline_summary': {\n",
    "                'urls_discovered': url_quantity,\n",
    "                'pdfs_acquired': pdf_quantity,\n",
    "                'templates_processed': len(extraction_map),\n",
    "                'total_extractions': sum(len(items) for items in extraction_map.values())\n",
    "            },\n",
    "            'template_details': {}\n",
    "        }\n",
    "        \n",
    "        # Build per-template statistics\n",
    "        for template_id, extraction_list in extraction_map.items():\n",
    "            values_extracted = sum(\n",
    "                1 for item in extraction_list \n",
    "                if item['extracted_value'] is not None\n",
    "            )\n",
    "            \n",
    "            confidence_scores = [\n",
    "                item['confidence_score'] \n",
    "                for item in extraction_list\n",
    "            ]\n",
    "            \n",
    "            mean_confidence = (\n",
    "                sum(confidence_scores) / len(confidence_scores) \n",
    "                if confidence_scores else 0.0\n",
    "            )\n",
    "            \n",
    "            coverage_percentage = (\n",
    "                (values_extracted / len(extraction_list) * 100) \n",
    "                if extraction_list else 0.0\n",
    "            )\n",
    "            \n",
    "            report_structure['template_details'][template_id] = {\n",
    "                'indicator_count': len(extraction_list),\n",
    "                'successful_extractions': values_extracted,\n",
    "                'coverage_percent': f\"{coverage_percentage:.1f}%\",\n",
    "                'average_confidence': f\"{mean_confidence:.3f}\",\n",
    "                'example_extractions': extraction_list[:3]\n",
    "            }\n",
    "        \n",
    "        # Persist report\n",
    "        report_file = output_location / 'vigone_extraction_report.json'\n",
    "        with open(report_file, 'w', encoding='utf-8') as report_stream:\n",
    "            json.dump(report_structure, report_stream, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nüìä Report saved to: {report_file}\")\n",
    "        return report_structure\n",
    "    \n",
    "    return build_extraction_report\n",
    "\n",
    "print(\"‚úì Reporting pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main orchestrator: chain all pipelines\n",
    "\n",
    "def execute_vigone_pipeline(\n",
    "    origin_url: str,\n",
    "    page_limit: int,\n",
    "    pdf_limit: int,\n",
    "    target_year: int,\n",
    "    pdf_storage: Path,\n",
    "    converted_storage: Path,\n",
    "    results_storage: Path,\n",
    "    template_source: Path,\n",
    "    state_file: Path\n",
    ") -> Dict:\n",
    "    \"\"\"Orchestrate complete extraction pipeline.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 15 + \"VIGONE DATA EXTRACTION PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Stage 1: Web Discovery\n",
    "    print(\"[STAGE 1/6] Web Discovery\")\n",
    "    print(\"-\" * 70)\n",
    "    crawler = construct_web_crawler(origin_url, page_limit)\n",
    "    pdf_urls = crawler()\n",
    "    print(f\"\\n‚úÖ Discovered {len(pdf_urls)} PDF URLs\\n\")\n",
    "    \n",
    "    # Stage 2: PDF Acquisition\n",
    "    print(\"[STAGE 2/6] PDF Acquisition\")\n",
    "    print(\"-\" * 70)\n",
    "    downloader = construct_pdf_downloader(pdf_storage, state_file)\n",
    "    pdf_hashes = downloader(pdf_urls, pdf_limit)\n",
    "    print(f\"\\n‚úÖ Acquired {len(pdf_hashes)} unique PDFs\\n\")\n",
    "    \n",
    "    # Stage 3: Marker Conversion\n",
    "    print(\"[STAGE 3/6] Marker Conversion\")\n",
    "    print(\"-\" * 70)\n",
    "    transformer = construct_marker_transformer(pdf_storage, converted_storage, state_file)\n",
    "    converted_documents = transformer()\n",
    "    print(f\"\\n‚úÖ Converted {len(converted_documents)} documents\\n\")\n",
    "    \n",
    "    # Stage 4: Indicator Extraction\n",
    "    print(\"[STAGE 4/6] Indicator Extraction\")\n",
    "    print(\"-\" * 70)\n",
    "    matcher = construct_indicator_matcher(converted_storage, template_source, target_year)\n",
    "    extracted_data = matcher()\n",
    "    total_matches = sum(len(items) for items in extracted_data.values())\n",
    "    print(f\"\\n‚úÖ Extracted {total_matches} indicator matches\\n\")\n",
    "    \n",
    "    # Stage 5: CSV Population\n",
    "    print(\"[STAGE 5/6] CSV Population\")\n",
    "    print(\"-\" * 70)\n",
    "    csv_writer = construct_csv_writer(template_source, results_storage, target_year)\n",
    "    populated_csvs = csv_writer(extracted_data)\n",
    "    print(f\"\\n‚úÖ Populated {len(populated_csvs)} CSV files\\n\")\n",
    "    \n",
    "    # Stage 6: Report Generation\n",
    "    print(\"[STAGE 6/6] Report Generation\")\n",
    "    print(\"-\" * 70)\n",
    "    report_builder = construct_report_builder(results_storage)\n",
    "    final_report = report_builder(extracted_data, len(pdf_hashes), len(pdf_urls))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" \" * 25 + \"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return final_report\n",
    "\n",
    "print(\"‚úì Orchestrator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test run with limited scope\n",
    "\n",
    "print(\"\\nüöÄ Starting quick test run...\\n\")\n",
    "\n",
    "test_results = execute_vigone_pipeline(\n",
    "    origin_url=vigone_origin,\n",
    "    page_limit=30,  # Reduced for testing\n",
    "    pdf_limit=20,   # Reduced for testing\n",
    "    target_year=vigone_year,\n",
    "    pdf_storage=vigone_pdf_storage,\n",
    "    converted_storage=vigone_converted_storage,\n",
    "    results_storage=vigone_results_storage,\n",
    "    template_source=vigone_template_source,\n",
    "    state_file=vigone_state_file\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \" * 28 + \"TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(json.dumps(test_results['pipeline_summary'], indent=2))\n",
    "print(\"\\nüìÅ Full report:\", vigone_results_storage / 'vigone_extraction_report.json')\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Full Production Run\n",
    "```python\n",
    "production_results = execute_vigone_pipeline(\n",
    "    origin_url=vigone_origin,\n",
    "    page_limit=vigone_page_limit,\n",
    "    pdf_limit=vigone_pdf_cap,\n",
    "    target_year=vigone_year,\n",
    "    pdf_storage=vigone_pdf_storage,\n",
    "    converted_storage=vigone_converted_storage,\n",
    "    results_storage=vigone_results_storage,\n",
    "    template_source=vigone_template_source,\n",
    "    state_file=vigone_state_file\n",
    ")\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**No PDFs discovered:**\n",
    "- Verify base URL is accessible\n",
    "- Increase `vigone_page_limit` parameter\n",
    "- Check if website structure changed\n",
    "- Inspect `manifest.json` for discovered URLs\n",
    "\n",
    "**Low extraction coverage:**\n",
    "- Review template indicator phrasing\n",
    "- Adjust similarity threshold in `calculate_token_similarity`\n",
    "- Check PDF quality (scanned vs. digital)\n",
    "- Verify Tesseract OCR is working\n",
    "\n",
    "**Marker conversion errors:**\n",
    "- Ensure sufficient disk space (>2GB free)\n",
    "- Check PDF file integrity\n",
    "- Install Italian language pack: `!apt-get install tesseract-ocr-ita`\n",
    "- Review conversion logs in terminal output\n",
    "\n",
    "**Resume interrupted pipeline:**\n",
    "- State is automatically saved in `manifest.json`\n",
    "- Re-run pipeline - it skips completed stages\n",
    "- Check `conversion_log` and `pdf_registry` in manifest\n",
    "\n",
    "### Output Files Structure\n",
    "\n",
    "```\n",
    "vigone_extraction/\n",
    "‚îú‚îÄ‚îÄ docs/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vigone_[hash].pdf        # Downloaded PDFs (hash-deduplicated)\n",
    "‚îú‚îÄ‚îÄ marker/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ vigone_[hash].json       # Structured JSON from Marker\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vigone_[hash].md         # Markdown text from Marker\n",
    "‚îú‚îÄ‚îÄ output/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ [template]_vigone_2024.csv  # Populated CSV templates\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vigone_extraction_report.json # Detailed statistics\n",
    "‚îî‚îÄ‚îÄ manifest.json                # Pipeline state tracking\n",
    "```\n",
    "\n",
    "### Advanced Configuration\n",
    "\n",
    "**Adjust similarity threshold:**\n",
    "```python\n",
    "# In calculate_token_similarity function\n",
    "if similarity > 0.3:  # Change threshold (0.0-1.0)\n",
    "```\n",
    "\n",
    "**Modify crawl depth:**\n",
    "```python\n",
    "# In construct_web_crawler function\n",
    "if current_depth > 3:  # Increase for deeper crawling\n",
    "```\n",
    "\n",
    "**Add custom unit patterns:**\n",
    "```python\n",
    "# In unit_inference_from_context function\n",
    "unit_rules = [\n",
    "    (r'your_pattern', 'your_unit'),\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

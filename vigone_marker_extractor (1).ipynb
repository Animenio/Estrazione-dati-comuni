{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vigone Municipality Data Harvester\n",
    "\n",
    "## Purpose\n",
    "This notebook orchestrates an end-to-end pipeline for extracting municipal indicators from Vigone (TO) website PDFs.\n",
    "\n",
    "## Prerequisites\n",
    "- **Google Drive Mount**: Required for accessing CSV templates and storing outputs\n",
    "- **Templates**: 6 CSV indicator templates must exist in Drive at configured path\n",
    "- **System**: Runs on Google Colab with sufficient disk space for PDF processing\n",
    "\n",
    "## Configuration\n",
    "- `ANNO_TARGET`: Target year for data extraction (default: 2024)\n",
    "- `MAX_PAGES`: Web crawl depth limit (default: 50)\n",
    "- `MAX_PDFS`: Maximum PDFs to process (default: 30)\n",
    "\n",
    "## Output Structure\n",
    "```\n",
    "/content/drive/MyDrive/vigone_extraction/\n",
    "├── docs/          # Downloaded PDFs\n",
    "├── marker/        # Converted JSON/Markdown\n",
    "├── output/        # Populated CSV templates\n",
    "└── manifest.json  # Resume state tracking\n",
    "```\n",
    "\n",
    "**Workflow**: Web Discovery → PDF Download → Marker Conversion → Indicator Extraction → CSV Population → Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def ensure_pip(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "for pkg in [\"requests\", \"beautifulsoup4\", \"pandas\", \"tenacity\", \"tqdm\", \"marker-pdf\", \"PyMuPDF\", \"pdf2image\", \"pytesseract\"]:\n",
    "    ensure_pip(pkg)\n",
    "\n",
    "subprocess.check_call([\"apt-get\", \"update\", \"-qq\"])\n",
    "subprocess.check_call([\"apt-get\", \"install\", \"-y\", \"-qq\", \"poppler-utils\", \"tesseract-ocr\", \"tesseract-ocr-ita\"])\n",
    "\n",
    "print(\"✅ Dependencies installed\")\n",
    "subprocess.run([\"marker_single\", \"--version\"], check=False)\n",
    "subprocess.run([\"marker_single\", \"--help\"], check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "comune_url = \"https://www.comune.vigone.to.it/\"\n",
    "comune_name = \"Vigone\"\n",
    "year = 2024\n",
    "\n",
    "candidate_template_paths = [\n",
    "    Path(\"/content/drive/MyDrive/dataset_dati_comuni\"),\n",
    "    Path(\"/content/drive/MyDrive/templates\"),\n",
    "]\n",
    "template_source = next((p for p in candidate_template_paths if p.exists()), candidate_template_paths[0])\n",
    "\n",
    "workspace = Path(f\"/content/drive/MyDrive/estrazione_dati_comuni/Comuni/{comune_name}/{year}\")\n",
    "docs_dir = workspace / \"docs\"\n",
    "parsed_dir = workspace / \"parsed\"\n",
    "cache_dir = workspace / \"cache\"\n",
    "output_dir = workspace / \"output\"\n",
    "state_path = workspace / \"manifest.json\"\n",
    "\n",
    "for p in [workspace, docs_dir, parsed_dir, cache_dir, output_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "required_templates = [\n",
    "    \"01_governo.csv\",\n",
    "    \"02_territorio_popolazione.csv\",\n",
    "    \"03_risultati_pillole.csv\",\n",
    "    \"04_servizi_civici.csv\",\n",
    "    \"05_rifiuti.csv\",\n",
    "    \"06_progetti.csv\",\n",
    "]\n",
    "\n",
    "if not Path('/content/drive').exists():\n",
    "    raise RuntimeError(\"Google Drive non risulta montato in /content/drive.\")\n",
    "\n",
    "missing = [x for x in required_templates if not (template_source / x).exists()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Template mancanti in {template_source}: {missing}\")\n",
    "\n",
    "print(f\"✅ Template source: {template_source}\")\n",
    "print(f\"✅ Workspace: {workspace}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from urllib.parse import urljoin, urlparse, urldefrag\n",
    "\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pdf2image import convert_from_path\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "USER_AGENT = \"Mozilla/5.0 (compatible; VigoneDataBot/2.0)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "def sha256_text(value: str) -> str:\n",
    "    return hashlib.sha256(value.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def sha256_bytes(value: bytes) -> str:\n",
    "    return hashlib.sha256(value).hexdigest()\n",
    "\n",
    "\n",
    "def now_iso() -> str:\n",
    "    return datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "\n",
    "def canonicalize_for_crawl(base_url: str, href: str) -> Optional[str]:\n",
    "    if not href:\n",
    "        return None\n",
    "    url = urldefrag(urljoin(base_url, href.strip()))[0]\n",
    "    p = urlparse(url)\n",
    "    if p.scheme not in {\"http\", \"https\"}:\n",
    "        return None\n",
    "    if p.netloc != urlparse(comune_url).netloc:\n",
    "        return None\n",
    "    if is_pdf_url(url):\n",
    "        return None\n",
    "    return url\n",
    "\n",
    "\n",
    "def canonicalize_pdf_link(base_url: str, href: str) -> Optional[str]:\n",
    "    if not href:\n",
    "        return None\n",
    "    url = urldefrag(urljoin(base_url, href.strip()))[0]\n",
    "    p = urlparse(url)\n",
    "    if p.scheme not in {\"http\", \"https\"}:\n",
    "        return None\n",
    "    if is_pdf_url(url):\n",
    "        return url\n",
    "    return None\n",
    "\n",
    "\n",
    "def is_pdf_url(url: str) -> bool:\n",
    "    return urlparse(url).path.lower().endswith(\".pdf\")\n",
    "\n",
    "\n",
    "def pick_markdown_generated(folder: Path) -> Optional[Path]:\n",
    "    mds = list(folder.rglob(\"*.md\"))\n",
    "    if not mds:\n",
    "        return None\n",
    "    return max(mds, key=lambda p: p.stat().st_size)\n",
    "\n",
    "\n",
    "def extract_number_near_text(text: str) -> Optional[str]:\n",
    "    m = re.search(r\"(?<!\\d)(\\d{1,3}(?:[\\.\\s]\\d{3})*(?:,\\d+)?|\\d+(?:[\\.,]\\d+)?)(?!\\d)\", text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def infer_unit(indicator: str, snippet: str) -> str:\n",
    "    t = f\"{indicator} {snippet}\".lower()\n",
    "    if \"%\" in t or \"percent\" in t:\n",
    "        return \"%\"\n",
    "    if \"euro\" in t or \"€\" in t:\n",
    "        return \"EUR\"\n",
    "    if \"ton\" in t:\n",
    "        return \"TON\"\n",
    "    return \"COUNT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifest state\n",
    "\n",
    "def read_manifest(path: Path) -> Dict:\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {\n",
    "        \"run\": {},\n",
    "        \"url_catalog\": {\"items\": [], \"total\": 0, \"updated_at\": None},\n",
    "        \"pdf_registry\": {},\n",
    "        \"pdf_index\": {},\n",
    "        \"conversion_log\": [],\n",
    "        \"extractions\": [],\n",
    "        \"metrics\": {\n",
    "            \"urls_discovered\": 0,\n",
    "            \"pdfs_acquired\": 0,\n",
    "            \"pdfs_parsed_ok\": 0,\n",
    "            \"pdfs_parsed_failed\": 0,\n",
    "            \"templates_found\": 0,\n",
    "            \"fields_total\": 0,\n",
    "            \"fields_filled\": 0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def write_manifest(path: Path, manifest: Dict) -> None:\n",
    "    manifest[\"updated_at\"] = now_iso()\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 - Web Discovery\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8))\n",
    "def fetch_url(url: str) -> requests.Response:\n",
    "    r = requests.get(url, timeout=25, headers={\"User-Agent\": USER_AGENT})\n",
    "    r.raise_for_status()\n",
    "    return r\n",
    "\n",
    "\n",
    "def discover_urls(seed_urls: List[str], max_pages: int = 300, max_depth: int = 4, sleep_s: float = 0.05) -> Tuple[List[str], Dict]:\n",
    "    visited = set()\n",
    "    queue = deque([(u, 0) for u in seed_urls])\n",
    "    discovered_pdf = set()\n",
    "    pdf_discarded = []\n",
    "    visited_log = []\n",
    "\n",
    "    while queue and len(visited) < max_pages:\n",
    "        url, depth = queue.popleft()\n",
    "        if depth > max_depth or url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        visited_log.append(url)\n",
    "\n",
    "        try:\n",
    "            resp = fetch_url(url)\n",
    "        except Exception as exc:\n",
    "            continue\n",
    "\n",
    "        ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        is_xml = \"xml\" in ctype or urlparse(url).path.lower().endswith(\".xml\")\n",
    "        parser = \"xml\" if is_xml else \"html.parser\"\n",
    "        soup = BeautifulSoup(resp.text, features=parser)\n",
    "\n",
    "        if is_xml:\n",
    "            for loc in soup.find_all(\"loc\"):\n",
    "                nxt = canonicalize_for_crawl(url, loc.get_text(strip=True))\n",
    "                if nxt and nxt not in visited:\n",
    "                    queue.append((nxt, depth + 1))\n",
    "            continue\n",
    "\n",
    "        for tag, attr in [(\"a\", \"href\"), (\"iframe\", \"src\"), (\"embed\", \"src\"), (\"object\", \"data\"), (\"link\", \"href\")]:\n",
    "            for node in soup.find_all(tag):\n",
    "                raw = node.get(attr)\n",
    "                if not raw:\n",
    "                    continue\n",
    "                pdf_link = canonicalize_pdf_link(url, raw)\n",
    "                if pdf_link:\n",
    "                    discovered_pdf.add(pdf_link)\n",
    "                else:\n",
    "                    reason = \"non_pdf_or_invalid\"\n",
    "                    pdf_discarded.append({\"from\": url, \"href\": raw, \"reason\": reason})\n",
    "                html_link = canonicalize_for_crawl(url, raw)\n",
    "                if html_link and html_link not in visited:\n",
    "                    queue.append((html_link, depth + 1))\n",
    "\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    diag = {\n",
    "        \"visited_top10\": visited_log[:10],\n",
    "        \"pdf_seen_discarded_top20\": pdf_discarded[:20],\n",
    "    }\n",
    "    return sorted(discovered_pdf), diag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 - PDF Acquisition\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=10))\n",
    "def download_pdf(url: str) -> bytes:\n",
    "    r = requests.get(url, timeout=45, headers={\"User-Agent\": USER_AGENT})\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "\n",
    "def acquire_pdfs(pdf_urls: List[str], manifest: Dict, max_pdfs: int = 120) -> Dict:\n",
    "    reg = manifest.setdefault(\"pdf_registry\", {})\n",
    "    idx = manifest.setdefault(\"pdf_index\", {})\n",
    "\n",
    "    def priority(u: str):\n",
    "        s = u.lower()\n",
    "        return 0 if str(year) in s or str(year - 1) in s else 1\n",
    "\n",
    "    acquired = 0\n",
    "    for url in sorted(pdf_urls, key=priority)[:max_pdfs]:\n",
    "        url_key = sha256_text(url)\n",
    "        existing = reg.get(url_key)\n",
    "        if existing and Path(existing.get(\"local_path\", \"\")).exists():\n",
    "            continue\n",
    "        try:\n",
    "            content = download_pdf(url)\n",
    "        except Exception:\n",
    "            continue\n",
    "        local = docs_dir / f\"{url_key}.pdf\"\n",
    "        with open(local, \"wb\") as f:\n",
    "            f.write(content)\n",
    "        doc_id = sha256_bytes(content)\n",
    "\n",
    "        if doc_id not in idx:\n",
    "            idx[doc_id] = {\n",
    "                \"source_url\": url,\n",
    "                \"local_path\": str(local),\n",
    "                \"url_key\": url_key,\n",
    "                \"filename_original\": Path(urlparse(url).path).name,\n",
    "            }\n",
    "            acquired += 1\n",
    "        reg[url_key] = {\"doc_id\": doc_id, \"local_path\": str(local), \"source_url\": url, \"updated_at\": now_iso()}\n",
    "\n",
    "    manifest[\"metrics\"][\"pdfs_acquired\"] = len(idx)\n",
    "    return manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3 - Marker + fallback\n",
    "\n",
    "def parse_pdf_with_fallback(manifest: Dict) -> Dict:\n",
    "    conversion_log = manifest.setdefault(\"conversion_log\", [])\n",
    "    idx = manifest.get(\"pdf_index\", {})\n",
    "    ok = 0\n",
    "    failed = 0\n",
    "\n",
    "    for doc_id, meta in tqdm(idx.items(), desc=\"Stage 3 parse\"):\n",
    "        pdf_path = Path(meta[\"local_path\"])\n",
    "        md_path = parsed_dir / f\"{doc_id}.md\"\n",
    "        meta_json = parsed_dir / f\"{doc_id}.meta.json\"\n",
    "\n",
    "        if md_path.exists():\n",
    "            ok += 1\n",
    "            continue\n",
    "\n",
    "        method, status, stdout_head, stderr_head = \"marker\", \"failed\", \"\", \"\"\n",
    "\n",
    "        try:\n",
    "            tmp_out = cache_dir / f\"marker_{doc_id}\"\n",
    "            if tmp_out.exists():\n",
    "                shutil.rmtree(tmp_out)\n",
    "            tmp_out.mkdir(parents=True, exist_ok=True)\n",
    "            proc = subprocess.run(\n",
    "                [\"marker_single\", str(pdf_path), \"--output_dir\", str(tmp_out)],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "            )\n",
    "            stdout_head = proc.stdout[:2000]\n",
    "            stderr_head = proc.stderr[:2000]\n",
    "            generated = pick_markdown_generated(tmp_out)\n",
    "            if proc.returncode == 0 and generated and generated.exists():\n",
    "                shutil.copy2(generated, md_path)\n",
    "                jsons = list(tmp_out.rglob(\"*.json\"))\n",
    "                if jsons:\n",
    "                    shutil.copy2(jsons[0], meta_json)\n",
    "                status = \"ok\"\n",
    "            else:\n",
    "                raise RuntimeError(\"marker failed\")\n",
    "        except Exception:\n",
    "            method = \"pymupdf\"\n",
    "            try:\n",
    "                doc = fitz.open(pdf_path)\n",
    "                text = \"\\n\\n\".join(page.get_text() for page in doc)\n",
    "                doc.close()\n",
    "                if len(text.strip()) < 300:\n",
    "                    method = \"ocr\"\n",
    "                    images = convert_from_path(str(pdf_path), first_page=1, last_page=2, dpi=170)\n",
    "                    ocr_text = []\n",
    "                    for im in images:\n",
    "                        ocr_text.append(pytesseract.image_to_string(im, lang=\"ita+eng\"))\n",
    "                    text = text + \"\\n\\n\" + \"\\n\".join(ocr_text)\n",
    "                if len(text.strip()) >= 20:\n",
    "                    md_path.write_text(text, encoding=\"utf-8\")\n",
    "                    status = \"ok\"\n",
    "                else:\n",
    "                    status = \"failed\"\n",
    "            except Exception as exc:\n",
    "                stderr_head = (stderr_head + \"\\n\" + str(exc))[:2000]\n",
    "                status = \"failed\"\n",
    "\n",
    "        conversion_log.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"source_url\": meta.get(\"source_url\"),\n",
    "            \"method\": method,\n",
    "            \"status\": status,\n",
    "            \"stderr_head\": stderr_head,\n",
    "            \"stdout_head\": stdout_head,\n",
    "            \"md_path\": str(md_path),\n",
    "            \"ts\": now_iso(),\n",
    "        })\n",
    "\n",
    "        if status == \"ok\":\n",
    "            ok += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "\n",
    "    manifest[\"metrics\"][\"pdfs_parsed_ok\"] = ok\n",
    "    manifest[\"metrics\"][\"pdfs_parsed_failed\"] = failed\n",
    "    return manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 - Indicator extraction (rule-based)\n",
    "\n",
    "def load_templates() -> Dict[str, pd.DataFrame]:\n",
    "    loaded = {}\n",
    "    for t in required_templates:\n",
    "        p = template_source / t\n",
    "        if not p.exists():\n",
    "            raise RuntimeError(f\"Template obbligatorio mancante: {p}\")\n",
    "        loaded[t] = pd.read_csv(p)\n",
    "    return loaded\n",
    "\n",
    "\n",
    "def get_indicator_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [c for c in df.columns if any(k in c.lower() for k in [\"indic\", \"voce\", \"descr\", \"nome\", \"item\"]) ]\n",
    "    return candidates[0] if candidates else df.columns[0]\n",
    "\n",
    "\n",
    "def extract_from_corpus(templates: Dict[str, pd.DataFrame], manifest: Dict) -> Dict[str, List[Dict]]:\n",
    "    corpus = []\n",
    "    for doc_id, meta in manifest.get(\"pdf_index\", {}).items():\n",
    "        md_path = parsed_dir / f\"{doc_id}.md\"\n",
    "        if md_path.exists():\n",
    "            txt = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            lines = txt.splitlines()\n",
    "            corpus.append((doc_id, meta.get(\"source_url\"), lines))\n",
    "\n",
    "    results = {}\n",
    "    fields_total = 0\n",
    "    fields_filled = 0\n",
    "\n",
    "    for name, df in templates.items():\n",
    "        ind_col = get_indicator_column(df)\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            indicator = str(row.get(ind_col, \"\")).strip()\n",
    "            if not indicator or indicator.lower() == \"nan\":\n",
    "                continue\n",
    "            fields_total += 1\n",
    "            best = None\n",
    "            terms = [w for w in re.findall(r\"[a-zA-Zàèéìòù0-9]+\", indicator.lower()) if len(w) > 2]\n",
    "            for doc_id, source_url, lines in corpus:\n",
    "                for i, line in enumerate(lines):\n",
    "                    l = line.lower()\n",
    "                    score = sum(1 for t in terms if t in l)\n",
    "                    if score <= 0:\n",
    "                        continue\n",
    "                    snippet = \"\\n\".join(lines[max(0, i-1): i+2])\n",
    "                    num = extract_number_near_text(snippet)\n",
    "                    cand = {\n",
    "                        \"indicator\": indicator,\n",
    "                        \"value\": num,\n",
    "                        \"unit\": infer_unit(indicator, snippet),\n",
    "                        \"year\": year,\n",
    "                        \"confidence\": round(min(0.95, 0.35 + score * 0.1 + (0.25 if num else 0.0)), 2),\n",
    "                        \"snippet\": snippet[:1200],\n",
    "                        \"source_url\": source_url,\n",
    "                        \"doc_id\": doc_id,\n",
    "                    }\n",
    "                    if best is None or cand[\"confidence\"] > best[\"confidence\"]:\n",
    "                        best = cand\n",
    "            if best is None:\n",
    "                best = {\n",
    "                    \"indicator\": indicator,\n",
    "                    \"value\": None,\n",
    "                    \"unit\": None,\n",
    "                    \"year\": year,\n",
    "                    \"confidence\": 0.0,\n",
    "                    \"snippet\": \"\",\n",
    "                    \"source_url\": None,\n",
    "                    \"doc_id\": None,\n",
    "                }\n",
    "            else:\n",
    "                fields_filled += 1 if best[\"value\"] is not None else 0\n",
    "            rows.append(best)\n",
    "        results[name] = rows\n",
    "\n",
    "    manifest[\"metrics\"][\"templates_found\"] = len(templates)\n",
    "    manifest[\"metrics\"][\"fields_total\"] = fields_total\n",
    "    manifest[\"metrics\"][\"fields_filled\"] = fields_filled\n",
    "    manifest[\"extractions\"] = [{\"template\": k, \"records\": v} for k, v in results.items()]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5 - CSV population\n",
    "\n",
    "def populate_csvs(templates: Dict[str, pd.DataFrame], extracted: Dict[str, List[Dict]]) -> List[Path]:\n",
    "    written = []\n",
    "    summary_rows = []\n",
    "\n",
    "    for template_name, df in templates.items():\n",
    "        out_df = df.copy()\n",
    "        ind_col = get_indicator_column(out_df)\n",
    "\n",
    "        year_col = str(year) if str(year) in out_df.columns else None\n",
    "        prev_col = str(year - 1) if str(year - 1) in out_df.columns else None\n",
    "\n",
    "        if year_col is None:\n",
    "            year_col = f\"Anno_{year}\"\n",
    "            out_df[year_col] = pd.NA\n",
    "        if prev_col is None and f\"Anno_{year-1}\" in out_df.columns:\n",
    "            prev_col = f\"Anno_{year-1}\"\n",
    "\n",
    "        map_ext = {r[\"indicator\"]: r for r in extracted.get(template_name, [])}\n",
    "\n",
    "        for idx, row in out_df.iterrows():\n",
    "            indicator = str(row.get(ind_col, \"\")).strip()\n",
    "            ext = map_ext.get(indicator)\n",
    "            if not ext:\n",
    "                continue\n",
    "            out_df.at[idx, year_col] = ext.get(\"value\")\n",
    "            summary_rows.append({\n",
    "                \"template\": template_name,\n",
    "                \"indicator\": indicator,\n",
    "                \"year\": year,\n",
    "                \"value\": ext.get(\"value\"),\n",
    "                \"unit\": ext.get(\"unit\"),\n",
    "                \"confidence\": ext.get(\"confidence\"),\n",
    "                \"source_url\": ext.get(\"source_url\"),\n",
    "                \"doc_id\": ext.get(\"doc_id\"),\n",
    "            })\n",
    "\n",
    "        out_name = template_name.replace(\".csv\", f\"_{comune_name.lower()}_{year}.csv\")\n",
    "        out_path = output_dir / out_name\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        written.append(out_path)\n",
    "\n",
    "    pd.DataFrame(summary_rows).to_csv(output_dir / \"summary.csv\", index=False)\n",
    "    return written\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 6 - report + manifest\n",
    "\n",
    "def build_report(manifest: Dict, diagnostics: Dict) -> Dict:\n",
    "    top_errors = [\n",
    "        {\n",
    "            \"doc_id\": x.get(\"doc_id\"),\n",
    "            \"source_url\": x.get(\"source_url\"),\n",
    "            \"method\": x.get(\"method\"),\n",
    "            \"stderr_head\": x.get(\"stderr_head\", \"\")[:500],\n",
    "        }\n",
    "        for x in manifest.get(\"conversion_log\", []) if x.get(\"status\") == \"failed\"\n",
    "    ][:10]\n",
    "\n",
    "    report = {\n",
    "        \"run\": {\n",
    "            \"comune\": comune_name,\n",
    "            \"comune_url\": comune_url,\n",
    "            \"year\": year,\n",
    "            \"paths\": {\n",
    "                \"workspace\": str(workspace),\n",
    "                \"docs\": str(docs_dir),\n",
    "                \"parsed\": str(parsed_dir),\n",
    "                \"cache\": str(cache_dir),\n",
    "                \"output\": str(output_dir),\n",
    "            },\n",
    "        },\n",
    "        \"metrics\": manifest.get(\"metrics\", {}),\n",
    "        \"diagnostics\": diagnostics,\n",
    "        \"top_errors\": top_errors,\n",
    "        \"generated_at\": now_iso(),\n",
    "    }\n",
    "\n",
    "    with open(workspace / \"report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main orchestrator\n",
    "\n",
    "def run_pipeline(max_pages: int = 300, max_depth: int = 4, max_pdfs: int = 120) -> Dict:\n",
    "    manifest = read_manifest(state_path)\n",
    "    manifest[\"run\"] = {\"comune\": comune_name, \"url\": comune_url, \"year\": year}\n",
    "\n",
    "    seed_urls = [\n",
    "        comune_url,\n",
    "        \"https://www.comune.vigone.to.it/ita/pnrr.aspx\",\n",
    "        \"https://www.comune.vigone.to.it/ita/amministrazione_trasparente.aspx\",\n",
    "    ]\n",
    "\n",
    "    pdf_urls, diagnostics = discover_urls(seed_urls, max_pages=max_pages, max_depth=max_depth)\n",
    "    manifest[\"url_catalog\"] = {\"items\": pdf_urls, \"total\": len(pdf_urls), \"updated_at\": now_iso()}\n",
    "    manifest[\"metrics\"][\"urls_discovered\"] = len(pdf_urls)\n",
    "\n",
    "    manifest = acquire_pdfs(pdf_urls, manifest, max_pdfs=max_pdfs)\n",
    "    manifest = parse_pdf_with_fallback(manifest)\n",
    "\n",
    "    templates = load_templates()\n",
    "    extracted = extract_from_corpus(templates, manifest)\n",
    "    written_files = populate_csvs(templates, extracted)\n",
    "\n",
    "    write_manifest(state_path, manifest)\n",
    "    report = build_report(manifest, diagnostics)\n",
    "\n",
    "    if manifest[\"metrics\"][\"templates_found\"] == 0:\n",
    "        raise RuntimeError(\"templates_found=0: controlla template_source e preflight\")\n",
    "    if manifest[\"metrics\"][\"pdfs_parsed_ok\"] == 0:\n",
    "        raise RuntimeError(\"pdfs_parsed_ok=0: controlla conversion_log in manifest\")\n",
    "\n",
    "    print(\"\\n===== FINAL SUMMARY =====\")\n",
    "    m = manifest[\"metrics\"]\n",
    "    print(f\"urls_discovered: {m['urls_discovered']}\")\n",
    "    print(f\"pdfs_acquired: {m['pdfs_acquired']}\")\n",
    "    print(f\"pdfs_parsed_ok/failed: {m['pdfs_parsed_ok']}/{m['pdfs_parsed_failed']}\")\n",
    "    print(f\"templates_found: {m['templates_found']}\")\n",
    "    print(f\"fields_total: {m['fields_total']}\")\n",
    "    print(f\"fields_filled: {m['fields_filled']}\")\n",
    "\n",
    "    if m[\"urls_discovered\"] <= 3:\n",
    "        print(\"\\n[DIAGNOSTICS] Top 10 URL visitate:\")\n",
    "        for u in diagnostics.get(\"visited_top10\", []):\n",
    "            print(\" -\", u)\n",
    "        print(\"\\n[DIAGNOSTICS] Top 20 link PDF visti/scartati:\")\n",
    "        for d in diagnostics.get(\"pdf_seen_discarded_top20\", []):\n",
    "            print(\" -\", d)\n",
    "\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test run (Vigone)\n",
    "report = run_pipeline(max_pages=300, max_depth=4, max_pdfs=120)\n",
    "print(json.dumps(report[\"metrics\"], indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Full Production Run\n",
    "```python\n",
    "production_results = execute_vigone_pipeline(\n",
    "    origin_url=vigone_origin,\n",
    "    page_limit=vigone_page_limit,\n",
    "    pdf_limit=vigone_pdf_cap,\n",
    "    target_year=vigone_year,\n",
    "    pdf_storage=vigone_pdf_storage,\n",
    "    converted_storage=vigone_converted_storage,\n",
    "    results_storage=vigone_results_storage,\n",
    "    template_source=vigone_template_source,\n",
    "    state_file=vigone_state_file\n",
    ")\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**No PDFs discovered:**\n",
    "- Verify base URL is accessible\n",
    "- Increase `vigone_page_limit` parameter\n",
    "- Check if website structure changed\n",
    "- Inspect `manifest.json` for discovered URLs\n",
    "\n",
    "**Low extraction coverage:**\n",
    "- Review template indicator phrasing\n",
    "- Adjust similarity threshold in `calculate_token_similarity`\n",
    "- Check PDF quality (scanned vs. digital)\n",
    "- Verify Tesseract OCR is working\n",
    "\n",
    "**Marker conversion errors:**\n",
    "- Ensure sufficient disk space (>2GB free)\n",
    "- Check PDF file integrity\n",
    "- Install Italian language pack: `!apt-get install tesseract-ocr-ita`\n",
    "- Review conversion logs in terminal output\n",
    "\n",
    "**Resume interrupted pipeline:**\n",
    "- State is automatically saved in `manifest.json`\n",
    "- Re-run pipeline - it skips completed stages\n",
    "- Check `conversion_log` and `pdf_registry` in manifest\n",
    "\n",
    "### Output Files Structure\n",
    "\n",
    "```\n",
    "vigone_extraction/\n",
    "├── docs/\n",
    "│   └── vigone_[hash].pdf        # Downloaded PDFs (hash-deduplicated)\n",
    "├── marker/\n",
    "│   ├── vigone_[hash].json       # Structured JSON from Marker\n",
    "│   └── vigone_[hash].md         # Markdown text from Marker\n",
    "├── output/\n",
    "│   ├── [template]_vigone_2024.csv  # Populated CSV templates\n",
    "│   └── vigone_extraction_report.json # Detailed statistics\n",
    "└── manifest.json                # Pipeline state tracking\n",
    "```\n",
    "\n",
    "### Advanced Configuration\n",
    "\n",
    "**Adjust similarity threshold:**\n",
    "```python\n",
    "# In calculate_token_similarity function\n",
    "if similarity > 0.3:  # Change threshold (0.0-1.0)\n",
    "```\n",
    "\n",
    "**Modify crawl depth:**\n",
    "```python\n",
    "# In construct_web_crawler function\n",
    "if current_depth > 3:  # Increase for deeper crawling\n",
    "```\n",
    "\n",
    "**Add custom unit patterns:**\n",
    "```python\n",
    "# In unit_inference_from_context function\n",
    "unit_rules = [\n",
    "    (r'your_pattern', 'your_unit'),\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
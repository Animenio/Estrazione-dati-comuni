{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "%pip -q install pandas requests beautifulsoup4 trafilatura scikit-learn rapidfuzz pdfplumber tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import csv\n",
        "import hashlib\n",
        "from collections import defaultdict, Counter\n",
        "from dataclasses import dataclass\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import trafilatura\n",
        "from tqdm import tqdm\n",
        "from rapidfuzz import fuzz\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Input parameters\n",
        "base_url = input('Inserisci base_url (es. https://comune.vigone.to.it/): ').strip()\n",
        "nome_comune = input('Inserisci nome_comune (opzionale ma consigliato): ').strip()\n",
        "if not nome_comune:\n",
        "    nome_comune = None\n",
        "\n",
        "YEARS_TO_FILL = [2023, 2024]\n",
        "ALLOW_EXTERNAL_OFFICIAL = input('ALLOW_EXTERNAL_OFFICIAL? [y/N]: ').strip().lower() in {'y', 'yes', 'true', '1'}\n",
        "\n",
        "input_dir = '/content/drive/MyDrive/vigone_csv/'\n",
        "output_dir = '/content/drive/MyDrive/vigone_output/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "parsed_domain = urlparse(base_url).netloc\n",
        "DOMAIN = parsed_domain\n",
        "COMUNE = nome_comune or ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Helpers: CSV parsing and missing detection\n",
        "YEAR_RE = re.compile(r'\b20\\d{2}\b')\n",
        "MISSING_RE = re.compile(r'(inserire|\\.\\.\\.|\bda compilare\b)', re.IGNORECASE)\n",
        "\n",
        "SECTION_HINTS = [\n",
        "    'Il Governo', 'Territorio e popolazione', 'I risultati in pillole', 'I servizi civici',\n",
        "    'Rifiuti urbani', 'Progetti', 'Servizi', 'Personale', 'Patrimonio', 'Rendiconto'\n",
        "]\n",
        "\n",
        "\n",
        "def is_missing(val):\n",
        "    if val is None:\n",
        "        return True\n",
        "    if isinstance(val, float) and math.isnan(val):\n",
        "        return True\n",
        "    if isinstance(val, str):\n",
        "        if not val.strip():\n",
        "            return True\n",
        "        if MISSING_RE.search(val):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def detect_header_row(df, max_rows=10):\n",
        "    best_row = 0\n",
        "    best_count = -1\n",
        "    for i in range(min(len(df), max_rows)):\n",
        "        row = df.iloc[i].astype(str).tolist()\n",
        "        count = sum(1 for cell in row if YEAR_RE.search(cell or ''))\n",
        "        if count > best_count:\n",
        "            best_count = count\n",
        "            best_row = i\n",
        "    return best_row\n",
        "\n",
        "\n",
        "def get_year_columns(df):\n",
        "    header_row = detect_header_row(df)\n",
        "    header = df.iloc[header_row].astype(str).tolist()\n",
        "    col_years = {}\n",
        "    for idx, cell in enumerate(header):\n",
        "        match = YEAR_RE.search(cell)\n",
        "        if match:\n",
        "            col_years[idx] = int(match.group(0))\n",
        "    if not col_years:\n",
        "        for idx in range(df.shape[1]):\n",
        "            col_vals = df.iloc[:15, idx].astype(str).tolist()\n",
        "            for val in col_vals:\n",
        "                match = YEAR_RE.search(val)\n",
        "                if match:\n",
        "                    col_years[idx] = int(match.group(0))\n",
        "                    break\n",
        "    return col_years, header_row\n",
        "\n",
        "\n",
        "def infer_row_label(row):\n",
        "    for cell in row:\n",
        "        text = str(cell).strip()\n",
        "        if text and not YEAR_RE.search(text):\n",
        "            return text\n",
        "    return ''\n",
        "\n",
        "\n",
        "def infer_section_context(df, row_idx):\n",
        "    for i in range(row_idx - 1, max(-1, row_idx - 5), -1):\n",
        "        row = df.iloc[i].astype(str).tolist()\n",
        "        row_text = ' '.join([c.strip() for c in row if c.strip()])\n",
        "        if not row_text:\n",
        "            continue\n",
        "        if any(h.lower() in row_text.lower() for h in SECTION_HINTS):\n",
        "            return row_text\n",
        "        if len(row_text) < 120 and sum(1 for c in row if c.strip()) <= 2:\n",
        "            return row_text\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Helpers: crawling and text extraction\n",
        "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Safari/537.36'}\n",
        "\n",
        "\n",
        "def fetch_url(url):\n",
        "    try:\n",
        "        resp = requests.get(url, headers=HEADERS, timeout=15)\n",
        "        if resp.status_code == 200:\n",
        "            return resp.text, resp.headers.get('content-type', '')\n",
        "    except requests.RequestException:\n",
        "        return None, None\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def extract_links(html, base):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    links = set()\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = urljoin(base, a['href'])\n",
        "        links.add(href.split('#')[0])\n",
        "    return links\n",
        "\n",
        "\n",
        "def is_same_domain(url, domain):\n",
        "    return urlparse(url).netloc == domain\n",
        "\n",
        "\n",
        "def get_sitemaps(base_url):\n",
        "    sitemap_urls = []\n",
        "    robots_url = urljoin(base_url, '/robots.txt')\n",
        "    text, _ = fetch_url(robots_url)\n",
        "    if text:\n",
        "        for line in text.splitlines():\n",
        "            if line.lower().startswith('sitemap:'):\n",
        "                sitemap_urls.append(line.split(':', 1)[1].strip())\n",
        "    if not sitemap_urls:\n",
        "        sitemap_urls.append(urljoin(base_url, '/sitemap.xml'))\n",
        "    return sitemap_urls\n",
        "\n",
        "\n",
        "def parse_sitemap(url):\n",
        "    text, _ = fetch_url(url)\n",
        "    if not text:\n",
        "        return []\n",
        "    soup = BeautifulSoup(text, 'xml')\n",
        "    urls = [loc.text.strip() for loc in soup.find_all('loc') if loc.text]\n",
        "    return urls\n",
        "\n",
        "\n",
        "def crawl_site(base_url, domain, max_pages=400):\n",
        "    visited = set()\n",
        "    queue = [base_url]\n",
        "    pages = []\n",
        "    pdfs = set()\n",
        "    while queue and len(visited) < max_pages:\n",
        "        url = queue.pop(0)\n",
        "        if url in visited or not is_same_domain(url, domain):\n",
        "            continue\n",
        "        visited.add(url)\n",
        "        html, ctype = fetch_url(url)\n",
        "        if not html:\n",
        "            continue\n",
        "        if 'pdf' in (ctype or '') or url.lower().endswith('.pdf'):\n",
        "            pdfs.add(url)\n",
        "            continue\n",
        "        pages.append({'url': url, 'html': html})\n",
        "        links = extract_links(html, url)\n",
        "        for link in links:\n",
        "            if link.lower().endswith('.pdf'):\n",
        "                pdfs.add(link)\n",
        "            elif is_same_domain(link, domain) and link not in visited:\n",
        "                queue.append(link)\n",
        "    return pages, list(pdfs)\n",
        "\n",
        "\n",
        "def extract_text_from_html(html):\n",
        "    downloaded = trafilatura.extract(html, include_comments=False, include_tables=True)\n",
        "    if downloaded:\n",
        "        return downloaded\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup.get_text('\n",
        "')\n",
        "\n",
        "\n",
        "def download_pdfs(pdf_urls, out_dir):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    downloaded = []\n",
        "    for url in tqdm(pdf_urls, desc='Downloading PDFs'):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=HEADERS, timeout=20)\n",
        "            if resp.status_code == 200:\n",
        "                fname = hashlib.md5(url.encode('utf-8')).hexdigest() + '.pdf'\n",
        "                path = os.path.join(out_dir, fname)\n",
        "                with open(path, 'wb') as f:\n",
        "                    f.write(resp.content)\n",
        "                downloaded.append((url, path))\n",
        "        except requests.RequestException:\n",
        "            continue\n",
        "    return downloaded\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(path):\n",
        "    texts = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            texts.append(page.extract_text() or '')\n",
        "    return '\n",
        "'.join(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Query Builder\n",
        "SYNONYMS = {\n",
        "    'delibera': ['delibera', 'deliberazione', 'deliberazioni'],\n",
        "    'consiglio': ['consiglio comunale', 'c.c.', 'consiglio'],\n",
        "    'giunta': ['giunta comunale', 'g.c.', 'giunta'],\n",
        "    'odg': ['ordine del giorno', 'o.d.g.'],\n",
        "    'rd': ['raccolta differenziata', 'rd'],\n",
        "    'umido': ['umido', 'organico'],\n",
        "    'multimateriale': ['multimateriale', 'vetro e lattine', 'vetro+lattine'],\n",
        "    'indifferenziato': ['indifferenziato', 'secco residuo'],\n",
        "}\n",
        "\n",
        "INURL_VARIANTS = [\n",
        "    'inurl:albo', 'inurl:atti', 'inurl:archivio', 'inurl:trasparenza'\n",
        "]\n",
        "\n",
        "FILETYPE_VARIANTS = [\n",
        "    'filetype:pdf', 'filetype:doc', 'filetype:docx', 'filetype:xls', 'filetype:xlsx'\n",
        "]\n",
        "\n",
        "QUERY_TEMPLATES = {\n",
        "    'DELIBERE_CC': [\n",
        "        'site:{DOMAIN} \"VERBALE DI DELIBERAZIONE\" \"DEL CONSIGLIO COMUNALE\" {YEAR}',\n",
        "        'site:{DOMAIN} \"Deliberazione C.C.\" (\"n.\" OR \"N.\") {YEAR} {COMUNE}',\n",
        "        'site:{DOMAIN} \"L'anno duemilaventitré\" \"DEL CONSIGLIO COMUNALE\" \"N.\"',\n",
        "        'site:{DOMAIN} \"L'anno duemilaventiquattro\" \"DEL CONSIGLIO COMUNALE\" \"N.\"',\n",
        "    ],\n",
        "    'DELIBERE_GC': [\n",
        "        'site:{DOMAIN} \"VERBALE DI DELIBERAZIONE\" \"DELLA GIUNTA COMUNALE\" {YEAR}',\n",
        "        'site:{DOMAIN} \"Deliberazione G.C.\" (\"n.\" OR \"N.\") {YEAR} {COMUNE}',\n",
        "        'site:{DOMAIN} \"L'anno duemilaventitré\" \"DELLA GIUNTA COMUNALE\" \"N.\"',\n",
        "        'site:{DOMAIN} \"L'anno duemilaventiquattro\" \"DELLA GIUNTA COMUNALE\" \"N.\"',\n",
        "    ],\n",
        "    'SEDUTE_CC': [\n",
        "        'site:{DOMAIN} \"CONVOCAZIONE\" \"CONSIGLIO COMUNALE\" {YEAR}',\n",
        "        'site:{DOMAIN} (\"ORDINE DEL GIORNO\" OR \"O.D.G.\") \"CONSIGLIO COMUNALE\" {YEAR}',\n",
        "    ],\n",
        "    'SEDUTE_GC': [\n",
        "        'site:{DOMAIN} \"CONVOCAZIONE\" \"GIUNTA COMUNALE\" {YEAR}',\n",
        "        'site:{DOMAIN} (\"ORDINE DEL GIORNO\" OR \"O.D.G.\") \"GIUNTA COMUNALE\" {YEAR}',\n",
        "    ],\n",
        "    'PERSONALE': [\n",
        "        'site:{DOMAIN} \"personale in servizio\" (\"31 dicembre {YEAR}\" OR \"{YEAR}\")',\n",
        "        'site:{DOMAIN} (\"Relazione sulla performance\" OR \"PIAO\" OR \"Piano Integrato\") {YEAR} personale',\n",
        "        'site:{DOMAIN} (\"Conto annuale del personale\" OR \"dotazione organica\" OR \"fabbisogni\") {YEAR}',\n",
        "    ],\n",
        "    'ORGANIGRAMMA': [\n",
        "        'site:{DOMAIN} (organigramma OR funzionigramma OR \"articolazione degli uffici\")',\n",
        "        'site:{DOMAIN} inurl:amministrazione (ufficio OR area OR settore)',\n",
        "        'site:{DOMAIN} (\"Area Tecnica\" OR \"Area Amministrativa\") \"Personale\"',\n",
        "    ],\n",
        "    'SERVIZIO_CIVILE': [\n",
        "        'site:{DOMAIN} \"servizio civile\" (volontari OR operatori OR \"Servizio Civile Universale\") {YEAR}',\n",
        "        'site:{DOMAIN} (bando OR graduatoria) \"servizio civile\" {YEAR}',\n",
        "        'site:{DOMAIN} \"Servizio Civile Universale\" {COMUNE}',\n",
        "    ],\n",
        "    'PERSONALE_FASCE': [\n",
        "        'site:{DOMAIN} (personale OR dipendenti) (genere OR maschi OR femmine) (\"fasce di età\" OR \"anni\")',\n",
        "        'site:{DOMAIN} filetype:pdf (\"conto annuale\" OR PIAO) (età OR fascia) {YEAR}',\n",
        "    ],\n",
        "    'DEMOGRAFIA': [\n",
        "        'site:{DOMAIN} \"Popolazione residente\" {YEAR} {COMUNE}',\n",
        "        'site:{DOMAIN} \"Bilancio demografico\" {YEAR} {COMUNE}',\n",
        "        'site:{DOMAIN} (nati OR morti OR popolazione) {YEAR} {COMUNE}',\n",
        "        'site:dati.istat.it \"{COMUNE}\" \"Popolazione residente\" {YEAR}',\n",
        "        'site:demo.istat.it \"{COMUNE}\" (\"bilancio demografico\" OR \"popolazione\") {YEAR}',\n",
        "        'site:dati.istat.it \"{COMUNE}\" (Nati OR Morti) {YEAR}',\n",
        "    ],\n",
        "    'PATRIMONIO_NETTO': [\n",
        "        'site:{DOMAIN} (rendiconto OR \"stato patrimoniale\") filetype:pdf {YEAR} (\"Patrimonio netto\" OR \"Ricchezza netta\")',\n",
        "    ],\n",
        "    'DEBITI': [\n",
        "        'site:{DOMAIN} \"stato patrimoniale\" filetype:pdf {YEAR} Debiti',\n",
        "    ],\n",
        "    'RISULTATO_ECONOMICO': [\n",
        "        'site:{DOMAIN} \"conto economico\" filetype:pdf {YEAR} (\"Risultato d'esercizio\" OR \"Risultato economico\")',\n",
        "    ],\n",
        "    'MISSIONE_INVESTIMENTI': [\n",
        "        'site:{DOMAIN} rendiconto filetype:pdf {YEAR} (\"Missione 09\" OR \"MISSIONE 9\") (\"conto capitale\" OR investimenti)',\n",
        "        'site:{DOMAIN} rendiconto filetype:pdf {YEAR} (\"Missione 04\" OR \"MISSIONE 4\") (\"conto capitale\" OR investimenti)',\n",
        "        'site:{DOMAIN} rendiconto filetype:pdf {YEAR} (\"Missione 12\" OR \"MISSIONE 12\") (\"conto capitale\" OR investimenti)',\n",
        "        'site:{DOMAIN} rendiconto filetype:pdf {YEAR} (\"Missione 05\" OR \"MISSIONE 5\") (\"conto capitale\" OR investimenti)',\n",
        "        'site:{DOMAIN} rendiconto filetype:pdf {YEAR} (\"Missione 06\" OR \"MISSIONE 6\") (\"conto capitale\" OR investimenti)',\n",
        "    ],\n",
        "    'BDAP_MISSIONI': [\n",
        "        'site:bdap-opendata.mef.gov.it {COMUNE} {YEAR} missione spesa',\n",
        "        'site:bdap-opendata.mef.gov.it {COMUNE} {YEAR} rendiconto',\n",
        "    ],\n",
        "    'ALIQUOTE_IMU': [\n",
        "        'site:finanze.gov.it \"aliquote IMU\" {COMUNE} {YEAR}',\n",
        "        'site:{DOMAIN} \"aliquote IMU\" {YEAR}',\n",
        "    ],\n",
        "    'ADDIZIONALE_IRPEF': [\n",
        "        'site:finanze.gov.it \"addizionale comunale\" \"IRPEF\" {COMUNE} {YEAR}',\n",
        "        'site:{DOMAIN} \"addizionale comunale IRPEF\" {YEAR}',\n",
        "    ],\n",
        "    'SOCIAL': [\n",
        "        'site:{DOMAIN} (facebook OR instagram OR youtube OR telegram) {COMUNE}',\n",
        "        'site:{DOMAIN} \"Facebook\" {COMUNE}',\n",
        "        'site:{DOMAIN} \"Instagram\" {COMUNE}',\n",
        "    ],\n",
        "    'SERVIZI_CIVICI': [\n",
        "        'site:{DOMAIN} (\"polizia locale\" OR \"comando\") {YEAR} (relazione OR report) filetype:pdf',\n",
        "        'site:{DOMAIN} \"art. 208\" {YEAR} filetype:pdf',\n",
        "        'site:{DOMAIN} (CILA OR SCIA OR \"permesso di costruire\" OR PDC) {YEAR}',\n",
        "        'site:{DOMAIN} (manutenzione OR \"verde pubblico\" OR potatura OR disinfestazione) {YEAR} filetype:pdf',\n",
        "        'site:{DOMAIN} biblioteca {YEAR} (MLOL OR ebook OR accessi)',\n",
        "    ],\n",
        "    'RIFIUTI': [\n",
        "        'site:{DOMAIN} {YEAR} (rifiuti OR \"igiene urbana\") (\"raccolta differenziata\" OR RD) (kg OR tonnellate OR \"%\") filetype:pdf',\n",
        "    ],\n",
        "    'RIFIUTI_EXTERNAL': [\n",
        "        'site:catasto-rifiuti.isprambiente.it \"{COMUNE}\" {YEAR} \"raccolta differenziata\"',\n",
        "        'site:isprambiente.gov.it filetype:pdf \"Rapporto rifiuti urbani\" {YEAR} {COMUNE}',\n",
        "    ],\n",
        "    'PROGETTI': [\n",
        "        'site:{DOMAIN} (PNRR OR \"Italia Domani\") (CUP OR \"Codice Unico di Progetto\") filetype:pdf',\n",
        "        'site:{DOMAIN} (\"Programma triennale\" OR DUP) {YEAR} (opere OR interventi OR investimenti) filetype:pdf',\n",
        "    ],\n",
        "    'GENERIC': [\n",
        "        'site:{DOMAIN} {COMUNE} {YEAR} {LABEL}',\n",
        "        'site:{DOMAIN} {LABEL} {YEAR} filetype:pdf',\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def categorize_cell(row_label, section_context):\n",
        "    label = (row_label or '').lower()\n",
        "    context = (section_context or '').lower()\n",
        "    if 'imu' in label or 'aliquota' in label:\n",
        "        return 'ALIQUOTE_IMU'\n",
        "    if 'irpef' in label or 'addizionale' in label:\n",
        "        return 'ADDIZIONALE_IRPEF'\n",
        "    if 'delib' in label and 'consiglio' in label:\n",
        "        return 'DELIBERE_CC'\n",
        "    if 'delib' in label and 'giunta' in label:\n",
        "        return 'DELIBERE_GC'\n",
        "    if 'sedut' in label and 'consiglio' in label:\n",
        "        return 'SEDUTE_CC'\n",
        "    if 'sedut' in label and 'giunta' in label:\n",
        "        return 'SEDUTE_GC'\n",
        "    if 'personale' in label and ('età' in label or 'fasce' in label):\n",
        "        return 'PERSONALE_FASCE'\n",
        "    if 'personale' in label or 'dipend' in label:\n",
        "        return 'PERSONALE'\n",
        "    if 'organigramma' in label or 'uffici' in label or 'struttura' in label:\n",
        "        return 'ORGANIGRAMMA'\n",
        "    if 'servizio civile' in label:\n",
        "        return 'SERVIZIO_CIVILE'\n",
        "    if 'popolazione' in label or 'nati' in label or 'morti' in label or 'demograf' in label:\n",
        "        return 'DEMOGRAFIA'\n",
        "    if 'patrimonio' in label:\n",
        "        return 'PATRIMONIO_NETTO'\n",
        "    if 'debiti' in label:\n",
        "        return 'DEBITI'\n",
        "    if 'risultato' in label and 'econom' in label:\n",
        "        return 'RISULTATO_ECONOMICO'\n",
        "    if 'missione' in label or 'investimenti' in label:\n",
        "        return 'MISSIONE_INVESTIMENTI'\n",
        "    if 'facebook' in label or 'instagram' in label or 'youtube' in label or 'social' in label:\n",
        "        return 'SOCIAL'\n",
        "    if 'rifiuti' in label or 'raccolta' in label or 'rd' in label:\n",
        "        return 'RIFIUTI'\n",
        "    if 'progetto' in label or 'pnrr' in label or 'cup' in label:\n",
        "        return 'PROGETTI'\n",
        "    if 'polizia' in label or 'cila' in label or 'scia' in label or 'biblioteca' in label:\n",
        "        return 'SERVIZI_CIVICI'\n",
        "    if 'territorio' in context or 'popolazione' in context:\n",
        "        return 'DEMOGRAFIA'\n",
        "    return 'GENERIC'\n",
        "\n",
        "\n",
        "def build_queries(category, domain, comune, year, row_label, allow_external):\n",
        "    templates = list(QUERY_TEMPLATES.get(category, []))\n",
        "    if category == 'RIFIUTI' and allow_external:\n",
        "        templates.extend(QUERY_TEMPLATES.get('RIFIUTI_EXTERNAL', []))\n",
        "    if category == 'DEMOGRAFIA' and not allow_external:\n",
        "        templates = [t for t in templates if '{DOMAIN}' in t]\n",
        "    if category in {'ALIQUOTE_IMU', 'ADDIZIONALE_IRPEF', 'BDAP_MISSIONI'} and not allow_external:\n",
        "        templates = [t for t in templates if '{DOMAIN}' in t]\n",
        "    if category == 'GENERIC':\n",
        "        templates = templates + [\n",
        "            'site:{DOMAIN} \"{LABEL}\" {YEAR} {COMUNE}',\n",
        "            'site:{DOMAIN} \"{LABEL}\" {YEAR} ' + ' OR '.join(INURL_VARIANTS),\n",
        "        ]\n",
        "    if year != 2023:\n",
        "        templates = [t for t in templates if 'duemilaventitré' not in t]\n",
        "    if year != 2024:\n",
        "        templates = [t for t in templates if 'duemilaventiquattro' not in t]\n",
        "    expanded = []\n",
        "    for template in templates:\n",
        "        expanded.append(template)\n",
        "        if 'site:{DOMAIN}' in template:\n",
        "            for inurl in INURL_VARIANTS:\n",
        "                expanded.append(f\"{template} {inurl}\")\n",
        "            for ftype in FILETYPE_VARIANTS:\n",
        "                expanded.append(f\"{template} {ftype}\")\n",
        "    formatted = []\n",
        "    for q in expanded:\n",
        "        formatted.append(q.format(DOMAIN=domain, YEAR=year, COMUNE=comune or '', LABEL=row_label))\n",
        "    unique = []\n",
        "    seen = set()\n",
        "    for q in formatted:\n",
        "        q = ' '.join(q.split())\n",
        "        if q and q not in seen:\n",
        "            seen.add(q)\n",
        "            unique.append(q)\n",
        "    return unique\n",
        "\n",
        "\n",
        "def query_priority(query):\n",
        "    score = 1\n",
        "    if 'filetype:pdf' in query:\n",
        "        score += 2\n",
        "    if 'rendiconto' in query or 'stato patrimoniale' in query or 'verbale di deliberazione' in query:\n",
        "        score += 2\n",
        "    return score\n",
        "\n",
        "\n",
        "def query_to_text(query):\n",
        "    cleaned = re.sub(r'\b(site:|filetype:|inurl:)\\S+', '', query)\n",
        "    cleaned = cleaned.replace('OR', ' ')\n",
        "    cleaned = cleaned.replace('(', ' ').replace(')', ' ')\n",
        "    cleaned = cleaned.replace('\"', ' ')\n",
        "    return ' '.join(cleaned.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Build corpus and index\n",
        "print('Crawling domain:', DOMAIN)\n",
        "\n",
        "all_urls = set()\n",
        "for sitemap in get_sitemaps(base_url):\n",
        "    all_urls.update(parse_sitemap(sitemap))\n",
        "\n",
        "pages, pdfs = crawl_site(base_url, DOMAIN, max_pages=400)\n",
        "\n",
        "all_urls.update([p['url'] for p in pages])\n",
        "all_urls.update(pdfs)\n",
        "\n",
        "print('Pages:', len(pages))\n",
        "print('PDFs:', len(pdfs))\n",
        "\n",
        "text_docs = []\n",
        "for page in tqdm(pages, desc='Extracting HTML'):\n",
        "    text = extract_text_from_html(page['html'])\n",
        "    if text:\n",
        "        text_docs.append({'url': page['url'], 'text': text})\n",
        "\n",
        "pdf_dir = os.path.join(output_dir, 'pdf_cache')\n",
        "pdf_files = download_pdfs(pdfs, pdf_dir)\n",
        "for url, path in tqdm(pdf_files, desc='Extracting PDFs'):\n",
        "    try:\n",
        "        text = extract_text_from_pdf(path)\n",
        "        if text:\n",
        "            text_docs.append({'url': url, 'text': text})\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "print('Documents indexed:', len(text_docs))\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='italian', max_features=50000)\n",
        "corpus = [doc['text'] for doc in text_docs]\n",
        "if corpus:\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "else:\n",
        "    tfidf_matrix = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Retrieval and extraction\n",
        "NUM_RE = re.compile(r'(\\d+[\\.,]?\\d*)\\s?(€|euro|%|kg|tonnellate|t)?', re.IGNORECASE)\n",
        "\n",
        "\n",
        "def rerank_docs(candidate_indices, row_label, year):\n",
        "    scored = []\n",
        "    label = (row_label or '').lower()\n",
        "    for idx, score in candidate_indices:\n",
        "        text = text_docs[idx]['text'].lower()\n",
        "        bonus = 0\n",
        "        if str(year) in text:\n",
        "            bonus += 0.2\n",
        "        if label and label in text:\n",
        "            bonus += 0.2\n",
        "        scored.append((idx, score + bonus))\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scored\n",
        "\n",
        "\n",
        "def retrieve_docs(queries, top_k=5):\n",
        "    if tfidf_matrix is None:\n",
        "        return []\n",
        "    best_scores = defaultdict(float)\n",
        "    for q in queries:\n",
        "        q_text = query_to_text(q)\n",
        "        if not q_text:\n",
        "            continue\n",
        "        q_vec = vectorizer.transform([q_text])\n",
        "        scores = linear_kernel(q_vec, tfidf_matrix).flatten()\n",
        "        top_indices = scores.argsort()[::-1][:top_k]\n",
        "        for idx in top_indices:\n",
        "            best_scores[idx] = max(best_scores[idx], scores[idx])\n",
        "    ranked = sorted(best_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return ranked\n",
        "\n",
        "\n",
        "def extract_value_from_text(text, row_label, year):\n",
        "    label = (row_label or '').lower()\n",
        "    lines = text.split('\n",
        "')\n",
        "    candidates = []\n",
        "    for line in lines:\n",
        "        if str(year) in line or label in line.lower():\n",
        "            for match in NUM_RE.finditer(line):\n",
        "                num = match.group(1)\n",
        "                unit = match.group(2) or ''\n",
        "                snippet = line.strip()\n",
        "                candidates.append((num, unit, snippet))\n",
        "    if candidates:\n",
        "        return candidates[0]\n",
        "    for match in NUM_RE.finditer(text):\n",
        "        return match.group(1), match.group(2) or '', text[:200]\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Process CSVs and fill values\n",
        "input_files = [f for f in os.listdir(input_dir) if f.lower().endswith('.csv')]\n",
        "\n",
        "queries_audit_path = os.path.join(output_dir, 'queries_generated.csv')\n",
        "sources_path = os.path.join(output_dir, 'sources_long.csv')\n",
        "report_path = os.path.join(output_dir, 'run_report.md')\n",
        "\n",
        "query_rows = []\n",
        "source_rows = []\n",
        "report_lines = []\n",
        "\n",
        "for fname in input_files:\n",
        "    fpath = os.path.join(input_dir, fname)\n",
        "    df_raw = pd.read_csv(fpath, header=None, dtype=str, keep_default_na=False)\n",
        "    col_years, header_row = get_year_columns(df_raw)\n",
        "\n",
        "    df_filled = df_raw.copy()\n",
        "    filled_count = 0\n",
        "    total_targets = 0\n",
        "\n",
        "    for row_idx in range(header_row + 1, len(df_raw)):\n",
        "        row = df_raw.iloc[row_idx].tolist()\n",
        "        row_label = infer_row_label(row)\n",
        "        section_context = infer_section_context(df_raw, row_idx)\n",
        "        if not row_label:\n",
        "            continue\n",
        "        category = categorize_cell(row_label, section_context)\n",
        "\n",
        "        for col_idx, year in col_years.items():\n",
        "            if year not in YEARS_TO_FILL:\n",
        "                continue\n",
        "            cell_val = df_raw.iat[row_idx, col_idx]\n",
        "            if not is_missing(cell_val):\n",
        "                continue\n",
        "            total_targets += 1\n",
        "            queries = build_queries(category, DOMAIN, COMUNE, year, row_label, ALLOW_EXTERNAL_OFFICIAL)\n",
        "            for q in queries:\n",
        "                query_rows.append({\n",
        "                    'input_file': fname,\n",
        "                    'section': section_context,\n",
        "                    'row_label': row_label,\n",
        "                    'col_year': year,\n",
        "                    'query': q,\n",
        "                    'priority': query_priority(q),\n",
        "                    'notes': category\n",
        "                })\n",
        "            ranked = retrieve_docs(queries, top_k=8)\n",
        "            ranked = rerank_docs(ranked, row_label, year)\n",
        "            value = None\n",
        "            if ranked:\n",
        "                top_idx = ranked[0][0]\n",
        "                doc = text_docs[top_idx]\n",
        "                extracted = extract_value_from_text(doc['text'], row_label, year)\n",
        "                if extracted:\n",
        "                    num, unit, snippet = extracted\n",
        "                    value = f\"{num} {unit}\".strip()\n",
        "                    source_rows.append({\n",
        "                        'input_file': fname,\n",
        "                        'row_label': row_label,\n",
        "                        'section': section_context,\n",
        "                        'year': year,\n",
        "                        'value': value,\n",
        "                        'source_url': doc['url'],\n",
        "                        'snippet': snippet[:400]\n",
        "                    })\n",
        "            if value:\n",
        "                df_filled.iat[row_idx, col_idx] = value\n",
        "                filled_count += 1\n",
        "            else:\n",
        "                source_rows.append({\n",
        "                    'input_file': fname,\n",
        "                    'row_label': row_label,\n",
        "                    'section': section_context,\n",
        "                    'year': year,\n",
        "                    'value': '',\n",
        "                    'source_url': '',\n",
        "                    'snippet': 'NOT_FOUND'\n",
        "                })\n",
        "\n",
        "    output_file = os.path.join(output_dir, fname.replace('.csv', '_filled.csv'))\n",
        "    df_filled.to_csv(output_file, header=False, index=False)\n",
        "    report_lines.append(f\"## {fname}\")\n",
        "    report_lines.append(f\"Targets: {total_targets}, Filled: {filled_count}\")\n",
        "\n",
        "if query_rows:\n",
        "    pd.DataFrame(query_rows).to_csv(queries_audit_path, index=False)\n",
        "if source_rows:\n",
        "    pd.DataFrame(source_rows).to_csv(sources_path, index=False)\n",
        "\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    f.write('\n",
        "'.join(report_lines))\n",
        "\n",
        "print('Done. Outputs saved to', output_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Estrazione dati comuni",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}